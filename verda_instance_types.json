[
  {
    "best_for": [],
    "cpu": {
      "description": "32 CPU",
      "number_of_cores": 32
    },
    "deploy_warning": null,
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "1x GB300 SXM6 288GB",
      "number_of_gpus": 1
    },
    "gpu_memory": {
      "description": "288GB GPU RAM",
      "size_in_gigabytes": 288
    },
    "id": "fb300f00-0000-4000-8000-fb300f000001",
    "instance_type": "1GB300.32V",
    "memory": {
      "description": "225GB RAM",
      "size_in_gigabytes": 225
    },
    "model": "GB300",
    "name": "GB300 SXM6 288GB",
    "p2p": "1.8 TB/s",
    "price_per_hour": "5.450",
    "spot_price": "1.908",
    "dynamic_price": "5.450",
    "max_dynamic_price": "8.175",
    "serverless_price": "5.995",
    "serverless_spot_price": "2.098",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "NVIDIA",
    "display_name": null,
    "supported_os": []
  },
  {
    "best_for": [],
    "cpu": {
      "description": "64 CPU",
      "number_of_cores": 64
    },
    "deploy_warning": null,
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "2x GB300 SXM6 288GB",
      "number_of_gpus": 2
    },
    "gpu_memory": {
      "description": "576GB GPU RAM",
      "size_in_gigabytes": 576
    },
    "id": "fb300f00-0000-4000-8000-fb300f000002",
    "instance_type": "2GB300.64V",
    "memory": {
      "description": "450GB RAM",
      "size_in_gigabytes": 450
    },
    "model": "GB300",
    "name": "GB300 SXM6 288GB",
    "p2p": "1.8 TB/s",
    "price_per_hour": "10.90",
    "spot_price": "3.815",
    "dynamic_price": "10.90",
    "max_dynamic_price": "16.35",
    "serverless_price": "11.99",
    "serverless_spot_price": "4.197",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "NVIDIA",
    "display_name": null,
    "supported_os": []
  },
  {
    "best_for": [],
    "cpu": {
      "description": "128 CPU",
      "number_of_cores": 128
    },
    "deploy_warning": null,
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "4x GB300 SXM6 288GB",
      "number_of_gpus": 4
    },
    "gpu_memory": {
      "description": "1152GB GPU RAM",
      "size_in_gigabytes": 1152
    },
    "id": "fb300f00-0000-4000-8000-fb300f000004",
    "instance_type": "4GB300.128V",
    "memory": {
      "description": "900GB RAM",
      "size_in_gigabytes": 900
    },
    "model": "GB300",
    "name": "GB300 SXM6 288GB",
    "p2p": "1.8 TB/s",
    "price_per_hour": "21.80",
    "spot_price": "7.630",
    "dynamic_price": "21.80",
    "max_dynamic_price": "32.70",
    "serverless_price": "23.98",
    "serverless_spot_price": "8.393",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "NVIDIA",
    "display_name": null,
    "supported_os": []
  },
  {
    "best_for": [],
    "cpu": {
      "description": "30 CPU",
      "number_of_cores": 30
    },
    "deploy_warning": null,
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "1x B300 SXM6 262GB",
      "number_of_gpus": 1
    },
    "gpu_memory": {
      "description": "262GB GPU RAM",
      "size_in_gigabytes": 262
    },
    "id": "b300b300-b300-4000-8000-b300b300b301",
    "instance_type": "1B300.30V",
    "memory": {
      "description": "275GB RAM",
      "size_in_gigabytes": 275
    },
    "model": "B300",
    "name": "B300 SXM6 262GB",
    "p2p": "1.8 TB/s",
    "price_per_hour": "4.950",
    "spot_price": "1.733",
    "dynamic_price": "4.950",
    "max_dynamic_price": "7.425",
    "serverless_price": "5.445",
    "serverless_spot_price": "1.906",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "NVIDIA",
    "display_name": null,
    "supported_os": [
      "24.04.kubernetes1.31.1.cuda12.9.qcow2",
      "jupyter.cuda.12.8",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.8-cluster",
      "ubuntu-22.04-cuda-12.8-open",
      "ubuntu-22.04-cuda-12.8-open-docker",
      "ubuntu-22.04-cuda-13.0-open",
      "ubuntu-22.04-cuda-13.0-open-docker",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.8-open",
      "ubuntu-24.04-cuda-12.8-open-docker",
      "ubuntu-24.04-cuda-13.0-open",
      "ubuntu-24.04-cuda-13.0-open-docker"
    ]
  },
  {
    "best_for": [],
    "cpu": {
      "description": "60 CPU",
      "number_of_cores": 60
    },
    "deploy_warning": null,
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "2x B300 SXM6 262GB",
      "number_of_gpus": 2
    },
    "gpu_memory": {
      "description": "525GB GPU RAM",
      "size_in_gigabytes": 525
    },
    "id": "b300b300-b300-4000-8000-b300b300b302",
    "instance_type": "2B300.60V",
    "memory": {
      "description": "550GB RAM",
      "size_in_gigabytes": 550
    },
    "model": "B300",
    "name": "B300 SXM6 262GB",
    "p2p": "1.8 TB/s",
    "price_per_hour": "9.900",
    "spot_price": "3.465",
    "dynamic_price": "9.900",
    "max_dynamic_price": "14.85",
    "serverless_price": "10.89",
    "serverless_spot_price": "3.812",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "NVIDIA",
    "display_name": null,
    "supported_os": [
      "24.04.kubernetes1.31.1.cuda12.9.qcow2",
      "jupyter.cuda.12.8",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.8-cluster",
      "ubuntu-22.04-cuda-12.8-open",
      "ubuntu-22.04-cuda-12.8-open-docker",
      "ubuntu-22.04-cuda-13.0-open",
      "ubuntu-22.04-cuda-13.0-open-docker",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.8-open",
      "ubuntu-24.04-cuda-12.8-open-docker",
      "ubuntu-24.04-cuda-13.0-open",
      "ubuntu-24.04-cuda-13.0-open-docker"
    ]
  },
  {
    "best_for": [],
    "cpu": {
      "description": "120 CPU",
      "number_of_cores": 120
    },
    "deploy_warning": null,
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "4x B300 SXM6 262GB",
      "number_of_gpus": 4
    },
    "gpu_memory": {
      "description": "1050GB GPU RAM",
      "size_in_gigabytes": 1050
    },
    "id": "b300b300-b300-4000-8000-b300b300b304",
    "instance_type": "4B300.120V",
    "memory": {
      "description": "1100GB RAM",
      "size_in_gigabytes": 1100
    },
    "model": "B300",
    "name": "B300 SXM6 262GB",
    "p2p": "1.8 TB/s",
    "price_per_hour": "19.80",
    "spot_price": "6.930",
    "dynamic_price": "19.80",
    "max_dynamic_price": "29.70",
    "serverless_price": "21.78",
    "serverless_spot_price": "7.623",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "NVIDIA",
    "display_name": null,
    "supported_os": [
      "24.04.kubernetes1.31.1.cuda12.9.qcow2",
      "jupyter.cuda.12.8",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.8-cluster",
      "ubuntu-22.04-cuda-12.8-open",
      "ubuntu-22.04-cuda-12.8-open-docker",
      "ubuntu-22.04-cuda-13.0-open",
      "ubuntu-22.04-cuda-13.0-open-docker",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.8-open",
      "ubuntu-24.04-cuda-12.8-open-docker",
      "ubuntu-24.04-cuda-13.0-open",
      "ubuntu-24.04-cuda-13.0-open-docker"
    ]
  },
  {
    "best_for": [],
    "cpu": {
      "description": "240 CPU",
      "number_of_cores": 240
    },
    "deploy_warning": null,
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "8x B300 SXM6 262GB",
      "number_of_gpus": 8
    },
    "gpu_memory": {
      "description": "2100GB GPU RAM",
      "size_in_gigabytes": 2100
    },
    "id": "b300b300-b300-4000-8000-b300b300b308",
    "instance_type": "8B300.240V",
    "memory": {
      "description": "2200GB RAM",
      "size_in_gigabytes": 2200
    },
    "model": "B300",
    "name": "B300 SXM6 262GB",
    "p2p": "1.8 TB/s",
    "price_per_hour": "39.60",
    "spot_price": "13.86",
    "dynamic_price": "39.60",
    "max_dynamic_price": "59.40",
    "serverless_price": "43.56",
    "serverless_spot_price": "15.25",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "NVIDIA",
    "display_name": null,
    "supported_os": [
      "24.04.kubernetes1.31.1.cuda12.9.qcow2",
      "jupyter.cuda.12.8",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.8-cluster",
      "ubuntu-22.04-cuda-12.8-open",
      "ubuntu-22.04-cuda-12.8-open-docker",
      "ubuntu-22.04-cuda-13.0-open",
      "ubuntu-22.04-cuda-13.0-open-docker",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.8-open",
      "ubuntu-24.04-cuda-12.8-open-docker",
      "ubuntu-24.04-cuda-13.0-open",
      "ubuntu-24.04-cuda-13.0-open-docker"
    ]
  },
  {
    "best_for": [
      "Gargantuan ML models",
      "Multi-GPU training",
      "FP64 HPC",
      "NVLINK"
    ],
    "cpu": {
      "description": "30 CPU",
      "number_of_cores": 30
    },
    "deploy_warning": "Blackwell: Use \"Ubuntu 24.04 + CUDA 12.8 Open\" image type or install NVIDIA open drivers manually",
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "1x B200 SXM6 180GB",
      "number_of_gpus": 1
    },
    "gpu_memory": {
      "description": "180GB GPU RAM",
      "size_in_gigabytes": 180
    },
    "id": "b200100a-5b01-447d-a68e-0b694079a4fb",
    "instance_type": "1B200.30V",
    "memory": {
      "description": "184GB RAM",
      "size_in_gigabytes": 184
    },
    "model": "B200",
    "name": "B200 SXM6 180GB",
    "p2p": null,
    "price_per_hour": "3.990",
    "spot_price": "1.397",
    "dynamic_price": "3.990",
    "max_dynamic_price": "5.985",
    "serverless_price": "4.389",
    "serverless_spot_price": "1.536",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "NVIDIA",
    "display_name": null,
    "supported_os": [
      "24.04.kubernetes1.31.1.cuda12.9.qcow2",
      "jupyter.cuda.12.8",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.8-cluster",
      "ubuntu-22.04-cuda-12.8-open",
      "ubuntu-22.04-cuda-12.8-open-docker",
      "ubuntu-22.04-cuda-13.0-open",
      "ubuntu-22.04-cuda-13.0-open-docker",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.8-open",
      "ubuntu-24.04-cuda-12.8-open-docker",
      "ubuntu-24.04-cuda-13.0-open",
      "ubuntu-24.04-cuda-13.0-open-docker"
    ]
  },
  {
    "best_for": [
      "Gargantuan ML models",
      "Multi-GPU training",
      "FP64 HPC",
      "NVLINK"
    ],
    "cpu": {
      "description": "60 CPU",
      "number_of_cores": 60
    },
    "deploy_warning": "Blackwell: Use \"Ubuntu 24.04 + CUDA 12.8 Open\" image type or install NVIDIA open drivers manually",
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "2x B200 SXM6 180GB",
      "number_of_gpus": 2
    },
    "gpu_memory": {
      "description": "360GB GPU RAM",
      "size_in_gigabytes": 360
    },
    "id": "b200200a-5b01-447d-a68e-0b694079a4fb",
    "instance_type": "2B200.60V",
    "memory": {
      "description": "368GB RAM",
      "size_in_gigabytes": 368
    },
    "model": "B200",
    "name": "B200 SXM6 180GB",
    "p2p": "900 GB/s",
    "price_per_hour": "7.980",
    "spot_price": "2.793",
    "dynamic_price": "7.980",
    "max_dynamic_price": "11.97",
    "serverless_price": "8.778",
    "serverless_spot_price": "3.072",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "NVIDIA",
    "display_name": null,
    "supported_os": [
      "24.04.kubernetes1.31.1.cuda12.9.qcow2",
      "jupyter.cuda.12.8",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.8-cluster",
      "ubuntu-22.04-cuda-12.8-open",
      "ubuntu-22.04-cuda-12.8-open-docker",
      "ubuntu-22.04-cuda-13.0-open",
      "ubuntu-22.04-cuda-13.0-open-docker",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.8-open",
      "ubuntu-24.04-cuda-12.8-open-docker",
      "ubuntu-24.04-cuda-13.0-open",
      "ubuntu-24.04-cuda-13.0-open-docker"
    ]
  },
  {
    "best_for": [
      "Gargantuan ML models",
      "Multi-GPU training",
      "FP64 HPC",
      "NVLINK"
    ],
    "cpu": {
      "description": "120 CPU",
      "number_of_cores": 120
    },
    "deploy_warning": "Blackwell: Use \"Ubuntu 24.04 + CUDA 12.8 Open\" image type or install NVIDIA open drivers manually",
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "4x B200 SXM6 180GB",
      "number_of_gpus": 4
    },
    "gpu_memory": {
      "description": "720GB GPU RAM",
      "size_in_gigabytes": 720
    },
    "id": "b200400a-5b01-447d-a68e-0b694079a4fb",
    "instance_type": "4B200.120V",
    "memory": {
      "description": "736GB RAM",
      "size_in_gigabytes": 736
    },
    "model": "B200",
    "name": "B200 SXM6 180GB",
    "p2p": "1.8 TB/s",
    "price_per_hour": "15.96",
    "spot_price": "5.586",
    "dynamic_price": "15.96",
    "max_dynamic_price": "23.94",
    "serverless_price": "17.56",
    "serverless_spot_price": "6.145",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "NVIDIA",
    "display_name": null,
    "supported_os": [
      "24.04.kubernetes1.31.1.cuda12.9.qcow2",
      "jupyter.cuda.12.8",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.8-cluster",
      "ubuntu-22.04-cuda-12.8-open",
      "ubuntu-22.04-cuda-12.8-open-docker",
      "ubuntu-22.04-cuda-13.0-open",
      "ubuntu-22.04-cuda-13.0-open-docker",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.8-open",
      "ubuntu-24.04-cuda-12.8-open-docker",
      "ubuntu-24.04-cuda-13.0-open",
      "ubuntu-24.04-cuda-13.0-open-docker"
    ]
  },
  {
    "best_for": [
      "Gargantuan ML models",
      "Multi-GPU training",
      "FP64 HPC",
      "NVLINK"
    ],
    "cpu": {
      "description": "240 CPU",
      "number_of_cores": 240
    },
    "deploy_warning": "Blackwell: Use \"Ubuntu 24.04 + CUDA 12.8 Open\" image type or install NVIDIA open drivers manually",
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "8x B200 SXM6 180GB",
      "number_of_gpus": 8
    },
    "gpu_memory": {
      "description": "1440GB GPU RAM",
      "size_in_gigabytes": 1440
    },
    "id": "b200800a-5b01-447d-a68e-0b694079a4fb",
    "instance_type": "8B200.240V",
    "memory": {
      "description": "1472GB RAM",
      "size_in_gigabytes": 1472
    },
    "model": "B200",
    "name": "B200 SXM6 180GB",
    "p2p": "1.8 TB/s",
    "price_per_hour": "31.92",
    "spot_price": "11.17",
    "dynamic_price": "31.92",
    "max_dynamic_price": "47.88",
    "serverless_price": "35.11",
    "serverless_spot_price": "12.29",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "NVIDIA",
    "display_name": null,
    "supported_os": [
      "24.04.kubernetes1.31.1.cuda12.9.qcow2",
      "jupyter.cuda.12.8",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.8-cluster",
      "ubuntu-22.04-cuda-12.8-open",
      "ubuntu-22.04-cuda-12.8-open-docker",
      "ubuntu-22.04-cuda-13.0-open",
      "ubuntu-22.04-cuda-13.0-open-docker",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.8-open",
      "ubuntu-24.04-cuda-12.8-open-docker",
      "ubuntu-24.04-cuda-13.0-open",
      "ubuntu-24.04-cuda-13.0-open-docker"
    ]
  },
  {
    "best_for": [
      "Gargantuan ML models",
      "Multi-GPU training",
      "FP64 HPC",
      "NVLINK"
    ],
    "cpu": {
      "description": "44 CPU",
      "number_of_cores": 44
    },
    "deploy_warning": null,
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "1x H200 SXM5 141GB",
      "number_of_gpus": 1
    },
    "gpu_memory": {
      "description": "141GB GPU RAM",
      "size_in_gigabytes": 141
    },
    "id": "70d00949-222b-402d-9228-02f33505cc87",
    "instance_type": "1H200.141S.44V",
    "memory": {
      "description": "182GB RAM",
      "size_in_gigabytes": 182
    },
    "model": "H200",
    "name": "H200 SXM5 141GB",
    "p2p": null,
    "price_per_hour": "2.990",
    "spot_price": "1.047",
    "dynamic_price": "2.990",
    "max_dynamic_price": "4.485",
    "serverless_price": "3.289",
    "serverless_spot_price": "1.151",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "NVIDIA",
    "display_name": null,
    "supported_os": [
      "24.04.kubernetes1.31.1.cuda12.9.qcow2",
      "jupyter",
      "jupyter.cuda.12.8",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.0",
      "ubuntu-22.04-cuda-12.0-docker",
      "ubuntu-22.04-cuda-12.1",
      "ubuntu-22.04-cuda-12.1-docker",
      "ubuntu-22.04-cuda-12.3",
      "ubuntu-22.04-cuda-12.3-docker",
      "ubuntu-22.04-cuda-12.4",
      "ubuntu-22.04-cuda-12.4-cluster",
      "ubuntu-22.04-cuda-12.4-docker",
      "ubuntu-22.04-cuda-12.8-cluster",
      "ubuntu-22.04-cuda-12.8-open",
      "ubuntu-22.04-cuda-12.8-open-docker",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.6",
      "ubuntu-24.04-cuda-12.6-docker",
      "ubuntu-24.04-cuda-12.8-open",
      "ubuntu-24.04-cuda-12.8-open-docker",
      "ubuntu-24.04-cuda-13.0-open",
      "ubuntu-24.04-cuda-13.0-open-docker"
    ]
  },
  {
    "best_for": [
      "Gargantuan ML models",
      "Multi-GPU training",
      "FP64 HPC",
      "NVLINK"
    ],
    "cpu": {
      "description": "88 CPU",
      "number_of_cores": 88
    },
    "deploy_warning": null,
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "2x H200 SXM5 141GB",
      "number_of_gpus": 2
    },
    "gpu_memory": {
      "description": "282GB GPU RAM",
      "size_in_gigabytes": 282
    },
    "id": "cc3e7d54-a208-4eea-a845-8189a8874370",
    "instance_type": "2H200.141S.88V",
    "memory": {
      "description": "370GB RAM",
      "size_in_gigabytes": 370
    },
    "model": "H200",
    "name": "H200 SXM5 141GB",
    "p2p": "900 GB/s",
    "price_per_hour": "5.980",
    "spot_price": "2.093",
    "dynamic_price": "5.980",
    "max_dynamic_price": "8.970",
    "serverless_price": "6.578",
    "serverless_spot_price": "2.302",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "NVIDIA",
    "display_name": null,
    "supported_os": [
      "24.04.kubernetes1.31.1.cuda12.9.qcow2",
      "jupyter",
      "jupyter.cuda.12.8",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.0",
      "ubuntu-22.04-cuda-12.0-docker",
      "ubuntu-22.04-cuda-12.1",
      "ubuntu-22.04-cuda-12.1-docker",
      "ubuntu-22.04-cuda-12.3",
      "ubuntu-22.04-cuda-12.3-docker",
      "ubuntu-22.04-cuda-12.4",
      "ubuntu-22.04-cuda-12.4-cluster",
      "ubuntu-22.04-cuda-12.4-docker",
      "ubuntu-22.04-cuda-12.8-cluster",
      "ubuntu-22.04-cuda-12.8-open",
      "ubuntu-22.04-cuda-12.8-open-docker",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.6",
      "ubuntu-24.04-cuda-12.6-docker",
      "ubuntu-24.04-cuda-12.8-open",
      "ubuntu-24.04-cuda-12.8-open-docker",
      "ubuntu-24.04-cuda-13.0-open",
      "ubuntu-24.04-cuda-13.0-open-docker"
    ]
  },
  {
    "best_for": [
      "Gargantuan ML models",
      "Multi-GPU training",
      "FP64 HPC",
      "NVLINK"
    ],
    "cpu": {
      "description": "176 CPU",
      "number_of_cores": 176
    },
    "deploy_warning": null,
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "4x H200 SXM5 141GB",
      "number_of_gpus": 4
    },
    "gpu_memory": {
      "description": "564GB GPU RAM",
      "size_in_gigabytes": 564
    },
    "id": "07d715c2-59c3-49c6-8ab3-c3b5e060dd17",
    "instance_type": "4H200.141S.176V",
    "memory": {
      "description": "740GB RAM",
      "size_in_gigabytes": 740
    },
    "model": "H200",
    "name": "H200 SXM5 141GB",
    "p2p": "900 GB/s",
    "price_per_hour": "11.96",
    "spot_price": "4.186",
    "dynamic_price": "11.96",
    "max_dynamic_price": "17.94",
    "serverless_price": "13.16",
    "serverless_spot_price": "4.605",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "NVIDIA",
    "display_name": null,
    "supported_os": [
      "24.04.kubernetes1.31.1.cuda12.9.qcow2",
      "jupyter",
      "jupyter.cuda.12.8",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.0",
      "ubuntu-22.04-cuda-12.0-docker",
      "ubuntu-22.04-cuda-12.1",
      "ubuntu-22.04-cuda-12.1-docker",
      "ubuntu-22.04-cuda-12.3",
      "ubuntu-22.04-cuda-12.3-docker",
      "ubuntu-22.04-cuda-12.4",
      "ubuntu-22.04-cuda-12.4-cluster",
      "ubuntu-22.04-cuda-12.4-docker",
      "ubuntu-22.04-cuda-12.8-cluster",
      "ubuntu-22.04-cuda-12.8-open",
      "ubuntu-22.04-cuda-12.8-open-docker",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.6",
      "ubuntu-24.04-cuda-12.6-docker",
      "ubuntu-24.04-cuda-12.8-open",
      "ubuntu-24.04-cuda-12.8-open-docker",
      "ubuntu-24.04-cuda-13.0-open",
      "ubuntu-24.04-cuda-13.0-open-docker"
    ]
  },
  {
    "best_for": [
      "Gargantuan ML models",
      "Multi-GPU training",
      "FP64 HPC",
      "NVLINK"
    ],
    "cpu": {
      "description": "176 CPU",
      "number_of_cores": 176
    },
    "deploy_warning": null,
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "8x H200 SXM5 141GB",
      "number_of_gpus": 8
    },
    "gpu_memory": {
      "description": "1128GB GPU RAM",
      "size_in_gigabytes": 1128
    },
    "id": "ae4a1aae-550f-43e2-85b9-f6434235c575",
    "instance_type": "8H200.141S.176V",
    "memory": {
      "description": "1450GB RAM",
      "size_in_gigabytes": 1450
    },
    "model": "H200",
    "name": "H200 SXM5 141GB",
    "p2p": "900 GB/s",
    "price_per_hour": "23.92",
    "spot_price": "8.372",
    "dynamic_price": "23.92",
    "max_dynamic_price": "35.88",
    "serverless_price": "26.31",
    "serverless_spot_price": "9.210",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "NVIDIA",
    "display_name": null,
    "supported_os": [
      "24.04.kubernetes1.31.1.cuda12.9.qcow2",
      "jupyter",
      "jupyter.cuda.12.8",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.0",
      "ubuntu-22.04-cuda-12.0-docker",
      "ubuntu-22.04-cuda-12.1",
      "ubuntu-22.04-cuda-12.1-docker",
      "ubuntu-22.04-cuda-12.3",
      "ubuntu-22.04-cuda-12.3-docker",
      "ubuntu-22.04-cuda-12.4",
      "ubuntu-22.04-cuda-12.4-cluster",
      "ubuntu-22.04-cuda-12.4-docker",
      "ubuntu-22.04-cuda-12.8-cluster",
      "ubuntu-22.04-cuda-12.8-open",
      "ubuntu-22.04-cuda-12.8-open-docker",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.6",
      "ubuntu-24.04-cuda-12.6-docker",
      "ubuntu-24.04-cuda-12.8-open",
      "ubuntu-24.04-cuda-12.8-open-docker",
      "ubuntu-24.04-cuda-13.0-open",
      "ubuntu-24.04-cuda-13.0-open-docker"
    ]
  },
  {
    "best_for": [
      "Gargantuan ML models",
      "Multi-GPU training",
      "FP64 HPC",
      "NVLINK"
    ],
    "cpu": {
      "description": "30 CPU",
      "number_of_cores": 30
    },
    "deploy_warning": "H100: Use Nvidia driver 535 or higher for best performance",
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "1x H100 SXM5 80GB",
      "number_of_gpus": 1
    },
    "gpu_memory": {
      "description": "80GB GPU RAM",
      "size_in_gigabytes": 80
    },
    "id": "c01dd00d-0000-480b-ae4e-d429115d055b",
    "instance_type": "1H100.80S.30V",
    "memory": {
      "description": "120GB RAM",
      "size_in_gigabytes": 120
    },
    "model": "H100",
    "name": "H100 SXM5 80GB",
    "p2p": null,
    "price_per_hour": "2.290",
    "spot_price": "0.8015",
    "dynamic_price": "2.290",
    "max_dynamic_price": "3.435",
    "serverless_price": "2.519",
    "serverless_spot_price": "0.8817",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "NVIDIA",
    "display_name": null,
    "supported_os": [
      "24.04.kubernetes1.31.1.cuda12.9.qcow2",
      "jupyter",
      "jupyter.cuda.12.8",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.0",
      "ubuntu-22.04-cuda-12.0-docker",
      "ubuntu-22.04-cuda-12.1",
      "ubuntu-22.04-cuda-12.1-docker",
      "ubuntu-22.04-cuda-12.3",
      "ubuntu-22.04-cuda-12.3-docker",
      "ubuntu-22.04-cuda-12.4",
      "ubuntu-22.04-cuda-12.4-cluster",
      "ubuntu-22.04-cuda-12.4-docker",
      "ubuntu-22.04-cuda-12.8-cluster",
      "ubuntu-22.04-cuda-12.8-open",
      "ubuntu-22.04-cuda-12.8-open-docker",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.6",
      "ubuntu-24.04-cuda-12.6-docker",
      "ubuntu-24.04-cuda-12.8-open",
      "ubuntu-24.04-cuda-12.8-open-docker"
    ]
  },
  {
    "best_for": [
      "Gargantuan ML models",
      "Multi-GPU training",
      "FP64 HPC",
      "NVLINK"
    ],
    "cpu": {
      "description": "32 CPU",
      "number_of_cores": 32
    },
    "deploy_warning": "H100: Use Nvidia driver 535 or higher for best performance",
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "1x H100 SXM5 80GB",
      "number_of_gpus": 1
    },
    "gpu_memory": {
      "description": "80GB GPU RAM",
      "size_in_gigabytes": 80
    },
    "id": "c21cfe7e-37ca-4515-b760-003e12d09471",
    "instance_type": "1H100.80S.32V",
    "memory": {
      "description": "185GB RAM",
      "size_in_gigabytes": 185
    },
    "model": "H100",
    "name": "H100 SXM5 80GB",
    "p2p": null,
    "price_per_hour": "2.290",
    "spot_price": "0.8015",
    "dynamic_price": "2.290",
    "max_dynamic_price": "3.435",
    "serverless_price": "2.519",
    "serverless_spot_price": "0.8817",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "NVIDIA",
    "display_name": null,
    "supported_os": [
      "24.04.kubernetes1.31.1.cuda12.9.qcow2",
      "jupyter",
      "jupyter.cuda.12.8",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.0",
      "ubuntu-22.04-cuda-12.0-docker",
      "ubuntu-22.04-cuda-12.1",
      "ubuntu-22.04-cuda-12.1-docker",
      "ubuntu-22.04-cuda-12.3",
      "ubuntu-22.04-cuda-12.3-docker",
      "ubuntu-22.04-cuda-12.4",
      "ubuntu-22.04-cuda-12.4-cluster",
      "ubuntu-22.04-cuda-12.4-docker",
      "ubuntu-22.04-cuda-12.8-cluster",
      "ubuntu-22.04-cuda-12.8-open",
      "ubuntu-22.04-cuda-12.8-open-docker",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.6",
      "ubuntu-24.04-cuda-12.6-docker",
      "ubuntu-24.04-cuda-12.8-open",
      "ubuntu-24.04-cuda-12.8-open-docker"
    ]
  },
  {
    "best_for": [
      "Gargantuan ML models",
      "Multi-GPU training",
      "FP64 HPC",
      "NVLINK"
    ],
    "cpu": {
      "description": "80 CPU",
      "number_of_cores": 80
    },
    "deploy_warning": "H100: Use Nvidia driver 535 or higher for best performance",
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "2x H100 SXM5 80GB",
      "number_of_gpus": 2
    },
    "gpu_memory": {
      "description": "160GB GPU RAM",
      "size_in_gigabytes": 160
    },
    "id": "a2aa7335-0a53-4d85-815f-c19aea7f61fe",
    "instance_type": "2H100.80S.80V",
    "memory": {
      "description": "370GB RAM",
      "size_in_gigabytes": 370
    },
    "model": "H100",
    "name": "H100 SXM5 80GB",
    "p2p": "900 GB/s",
    "price_per_hour": "4.580",
    "spot_price": "1.603",
    "dynamic_price": "4.580",
    "max_dynamic_price": "6.870",
    "serverless_price": "5.038",
    "serverless_spot_price": "1.763",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "NVIDIA",
    "display_name": null,
    "supported_os": [
      "24.04.kubernetes1.31.1.cuda12.9.qcow2",
      "jupyter",
      "jupyter.cuda.12.8",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.0",
      "ubuntu-22.04-cuda-12.0-docker",
      "ubuntu-22.04-cuda-12.1",
      "ubuntu-22.04-cuda-12.1-docker",
      "ubuntu-22.04-cuda-12.3",
      "ubuntu-22.04-cuda-12.3-docker",
      "ubuntu-22.04-cuda-12.4",
      "ubuntu-22.04-cuda-12.4-cluster",
      "ubuntu-22.04-cuda-12.4-docker",
      "ubuntu-22.04-cuda-12.8-cluster",
      "ubuntu-22.04-cuda-12.8-open",
      "ubuntu-22.04-cuda-12.8-open-docker",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.6",
      "ubuntu-24.04-cuda-12.6-docker",
      "ubuntu-24.04-cuda-12.8-open",
      "ubuntu-24.04-cuda-12.8-open-docker"
    ]
  },
  {
    "best_for": [
      "Gargantuan ML models",
      "Multi-GPU training",
      "FP64 HPC",
      "NVLINK"
    ],
    "cpu": {
      "description": "176 CPU",
      "number_of_cores": 176
    },
    "deploy_warning": "H100: Use Nvidia driver 535 or higher for best performance",
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "4x H100 SXM5 80GB",
      "number_of_gpus": 4
    },
    "gpu_memory": {
      "description": "320GB GPU RAM",
      "size_in_gigabytes": 320
    },
    "id": "c01dd00d-0002-748b-ae4e-d429115d055b",
    "instance_type": "4H100.80S.176V",
    "memory": {
      "description": "740GB RAM",
      "size_in_gigabytes": 740
    },
    "model": "H100",
    "name": "H100 SXM5 80GB",
    "p2p": "900 GB/s",
    "price_per_hour": "9.160",
    "spot_price": "3.206",
    "dynamic_price": "9.160",
    "max_dynamic_price": "13.74",
    "serverless_price": "10.08",
    "serverless_spot_price": "3.527",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "NVIDIA",
    "display_name": null,
    "supported_os": [
      "24.04.kubernetes1.31.1.cuda12.9.qcow2",
      "jupyter",
      "jupyter.cuda.12.8",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.0",
      "ubuntu-22.04-cuda-12.0-docker",
      "ubuntu-22.04-cuda-12.1",
      "ubuntu-22.04-cuda-12.1-docker",
      "ubuntu-22.04-cuda-12.3",
      "ubuntu-22.04-cuda-12.3-docker",
      "ubuntu-22.04-cuda-12.4",
      "ubuntu-22.04-cuda-12.4-cluster",
      "ubuntu-22.04-cuda-12.4-docker",
      "ubuntu-22.04-cuda-12.8-cluster",
      "ubuntu-22.04-cuda-12.8-open",
      "ubuntu-22.04-cuda-12.8-open-docker",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.6",
      "ubuntu-24.04-cuda-12.6-docker",
      "ubuntu-24.04-cuda-12.8-open",
      "ubuntu-24.04-cuda-12.8-open-docker"
    ]
  },
  {
    "best_for": [
      "Gargantuan ML models",
      "Multi-GPU training",
      "FP64 HPC",
      "NVLINK"
    ],
    "cpu": {
      "description": "176 CPU",
      "number_of_cores": 176
    },
    "deploy_warning": "H100: Use Nvidia driver 535 or higher for best performance",
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "8x H100 SXM5 80GB",
      "number_of_gpus": 8
    },
    "gpu_memory": {
      "description": "640GB GPU RAM",
      "size_in_gigabytes": 640
    },
    "id": "c01dd00d-0003-4972-ae4e-d429115d055b",
    "instance_type": "8H100.80S.176V",
    "memory": {
      "description": "1480GB RAM",
      "size_in_gigabytes": 1480
    },
    "model": "H100",
    "name": "H100 SXM5 80GB",
    "p2p": "900 GB/s",
    "price_per_hour": "18.32",
    "spot_price": "6.412",
    "dynamic_price": "18.32",
    "max_dynamic_price": "27.48",
    "serverless_price": "20.15",
    "serverless_spot_price": "7.054",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "NVIDIA",
    "display_name": null,
    "supported_os": [
      "24.04.kubernetes1.31.1.cuda12.9.qcow2",
      "jupyter",
      "jupyter.cuda.12.8",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.0",
      "ubuntu-22.04-cuda-12.0-docker",
      "ubuntu-22.04-cuda-12.1",
      "ubuntu-22.04-cuda-12.1-docker",
      "ubuntu-22.04-cuda-12.3",
      "ubuntu-22.04-cuda-12.3-docker",
      "ubuntu-22.04-cuda-12.4",
      "ubuntu-22.04-cuda-12.4-cluster",
      "ubuntu-22.04-cuda-12.4-docker",
      "ubuntu-22.04-cuda-12.8-cluster",
      "ubuntu-22.04-cuda-12.8-open",
      "ubuntu-22.04-cuda-12.8-open-docker",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.6",
      "ubuntu-24.04-cuda-12.6-docker",
      "ubuntu-24.04-cuda-12.8-open",
      "ubuntu-24.04-cuda-12.8-open-docker"
    ]
  },
  {
    "best_for": [
      "Giant ML models",
      "Multi-GPU training",
      "FP64 calculations",
      "NVLINK"
    ],
    "cpu": {
      "description": "22 CPU",
      "number_of_cores": 22
    },
    "deploy_warning": null,
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "1x A100 SXM4 80GB",
      "number_of_gpus": 1
    },
    "gpu_memory": {
      "description": "80GB GPU RAM",
      "size_in_gigabytes": 80
    },
    "id": "a0000000-a5d2-4972-ae4e-d429115d055b",
    "instance_type": "1A100.22V",
    "memory": {
      "description": "120GB RAM",
      "size_in_gigabytes": 120
    },
    "model": "A100 80GB",
    "name": "A100 SXM4 80GB",
    "p2p": null,
    "price_per_hour": "1.290",
    "spot_price": "0.4515",
    "dynamic_price": "1.290",
    "max_dynamic_price": "1.935",
    "serverless_price": "1.419",
    "serverless_spot_price": "0.4967",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "NVIDIA",
    "display_name": null,
    "supported_os": [
      "24.04.kubernetes1.31.1.cuda12.9.qcow2",
      "jupyter",
      "jupyter.cuda.12.8",
      "ubuntu-20.04",
      "ubuntu-20.04-cuda-11.0",
      "ubuntu-20.04-cuda-11.1",
      "ubuntu-20.04-cuda-11.1-docker",
      "ubuntu-20.04-cuda-11.2",
      "ubuntu-20.04-cuda-11.4",
      "ubuntu-20.04-cuda-11.4-docker",
      "ubuntu-20.04-cuda-11.7",
      "ubuntu-20.04-cuda-11.7-docker",
      "ubuntu-20.04-cuda-12.4",
      "ubuntu-20.04-cuda-12.4-docker",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.0",
      "ubuntu-22.04-cuda-12.0-docker",
      "ubuntu-22.04-cuda-12.1",
      "ubuntu-22.04-cuda-12.1-docker",
      "ubuntu-22.04-cuda-12.3",
      "ubuntu-22.04-cuda-12.3-docker",
      "ubuntu-22.04-cuda-12.4",
      "ubuntu-22.04-cuda-12.4-cluster",
      "ubuntu-22.04-cuda-12.4-docker",
      "ubuntu-22.04-cuda-12.8-cluster",
      "ubuntu-22.04-cuda-12.8-open",
      "ubuntu-22.04-cuda-12.8-open-docker",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.6",
      "ubuntu-24.04-cuda-12.6-docker",
      "ubuntu-24.04-cuda-12.8-open",
      "ubuntu-24.04-cuda-12.8-open-docker"
    ]
  },
  {
    "best_for": [
      "Giant ML models",
      "Multi-GPU training",
      "FP64 calculations",
      "NVLINK"
    ],
    "cpu": {
      "description": "44 CPU",
      "number_of_cores": 44
    },
    "deploy_warning": null,
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "2x A100 SXM4 80GB",
      "number_of_gpus": 2
    },
    "gpu_memory": {
      "description": "160GB GPU RAM",
      "size_in_gigabytes": 160
    },
    "id": "a0000001-a5d2-4972-ae4e-d429115d055b",
    "instance_type": "2A100.44V",
    "memory": {
      "description": "240GB RAM",
      "size_in_gigabytes": 240
    },
    "model": "A100 80GB",
    "name": "A100 SXM4 80GB",
    "p2p": "100 GB/s",
    "price_per_hour": "2.580",
    "spot_price": "0.9030",
    "dynamic_price": "2.580",
    "max_dynamic_price": "3.870",
    "serverless_price": "2.838",
    "serverless_spot_price": "0.9934",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "NVIDIA",
    "display_name": null,
    "supported_os": [
      "24.04.kubernetes1.31.1.cuda12.9.qcow2",
      "jupyter",
      "jupyter.cuda.12.8",
      "ubuntu-20.04",
      "ubuntu-20.04-cuda-11.0",
      "ubuntu-20.04-cuda-11.1",
      "ubuntu-20.04-cuda-11.1-docker",
      "ubuntu-20.04-cuda-11.2",
      "ubuntu-20.04-cuda-11.4",
      "ubuntu-20.04-cuda-11.4-docker",
      "ubuntu-20.04-cuda-11.7",
      "ubuntu-20.04-cuda-11.7-docker",
      "ubuntu-20.04-cuda-12.4",
      "ubuntu-20.04-cuda-12.4-docker",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.0",
      "ubuntu-22.04-cuda-12.0-docker",
      "ubuntu-22.04-cuda-12.1",
      "ubuntu-22.04-cuda-12.1-docker",
      "ubuntu-22.04-cuda-12.3",
      "ubuntu-22.04-cuda-12.3-docker",
      "ubuntu-22.04-cuda-12.4",
      "ubuntu-22.04-cuda-12.4-cluster",
      "ubuntu-22.04-cuda-12.4-docker",
      "ubuntu-22.04-cuda-12.8-cluster",
      "ubuntu-22.04-cuda-12.8-open",
      "ubuntu-22.04-cuda-12.8-open-docker",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.6",
      "ubuntu-24.04-cuda-12.6-docker",
      "ubuntu-24.04-cuda-12.8-open",
      "ubuntu-24.04-cuda-12.8-open-docker"
    ]
  },
  {
    "best_for": [
      "Giant ML models",
      "Multi-GPU training",
      "FP64 calculations",
      "NVLINK"
    ],
    "cpu": {
      "description": "88 CPU",
      "number_of_cores": 88
    },
    "deploy_warning": null,
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "4x A100 SXM4 80GB",
      "number_of_gpus": 4
    },
    "gpu_memory": {
      "description": "320GB GPU RAM",
      "size_in_gigabytes": 320
    },
    "id": "a0000002-a5d2-4972-ae4e-d429115d055b",
    "instance_type": "4A100.88V",
    "memory": {
      "description": "480GB RAM",
      "size_in_gigabytes": 480
    },
    "model": "A100 80GB",
    "name": "A100 SXM4 80GB",
    "p2p": "300 GB/s",
    "price_per_hour": "5.160",
    "spot_price": "1.806",
    "dynamic_price": "5.160",
    "max_dynamic_price": "7.740",
    "serverless_price": "5.676",
    "serverless_spot_price": "1.987",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "NVIDIA",
    "display_name": null,
    "supported_os": [
      "24.04.kubernetes1.31.1.cuda12.9.qcow2",
      "jupyter",
      "jupyter.cuda.12.8",
      "ubuntu-20.04",
      "ubuntu-20.04-cuda-11.0",
      "ubuntu-20.04-cuda-11.1",
      "ubuntu-20.04-cuda-11.1-docker",
      "ubuntu-20.04-cuda-11.2",
      "ubuntu-20.04-cuda-11.4",
      "ubuntu-20.04-cuda-11.4-docker",
      "ubuntu-20.04-cuda-11.7",
      "ubuntu-20.04-cuda-11.7-docker",
      "ubuntu-20.04-cuda-12.4",
      "ubuntu-20.04-cuda-12.4-docker",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.0",
      "ubuntu-22.04-cuda-12.0-docker",
      "ubuntu-22.04-cuda-12.1",
      "ubuntu-22.04-cuda-12.1-docker",
      "ubuntu-22.04-cuda-12.3",
      "ubuntu-22.04-cuda-12.3-docker",
      "ubuntu-22.04-cuda-12.4",
      "ubuntu-22.04-cuda-12.4-cluster",
      "ubuntu-22.04-cuda-12.4-docker",
      "ubuntu-22.04-cuda-12.8-cluster",
      "ubuntu-22.04-cuda-12.8-open",
      "ubuntu-22.04-cuda-12.8-open-docker",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.6",
      "ubuntu-24.04-cuda-12.6-docker",
      "ubuntu-24.04-cuda-12.8-open",
      "ubuntu-24.04-cuda-12.8-open-docker"
    ]
  },
  {
    "best_for": [
      "Giant ML models",
      "Multi-GPU training",
      "FP64 calculations",
      "NVLINK"
    ],
    "cpu": {
      "description": "176 CPU",
      "number_of_cores": 176
    },
    "deploy_warning": null,
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "8x A100 SXM4 80GB",
      "number_of_gpus": 8
    },
    "gpu_memory": {
      "description": "640GB GPU RAM",
      "size_in_gigabytes": 640
    },
    "id": "a0000003-a5d2-4972-ae4e-d429115d055b",
    "instance_type": "8A100.176V",
    "memory": {
      "description": "960GB RAM",
      "size_in_gigabytes": 960
    },
    "model": "A100 80GB",
    "name": "A100 SXM4 80GB",
    "p2p": "600 GB/s",
    "price_per_hour": "10.32",
    "spot_price": "3.612",
    "dynamic_price": "10.32",
    "max_dynamic_price": "15.48",
    "serverless_price": "11.35",
    "serverless_spot_price": "3.974",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "NVIDIA",
    "display_name": null,
    "supported_os": [
      "24.04.kubernetes1.31.1.cuda12.9.qcow2",
      "jupyter",
      "jupyter.cuda.12.8",
      "ubuntu-20.04",
      "ubuntu-20.04-cuda-11.0",
      "ubuntu-20.04-cuda-11.1",
      "ubuntu-20.04-cuda-11.1-docker",
      "ubuntu-20.04-cuda-11.2",
      "ubuntu-20.04-cuda-11.4",
      "ubuntu-20.04-cuda-11.4-docker",
      "ubuntu-20.04-cuda-11.7",
      "ubuntu-20.04-cuda-11.7-docker",
      "ubuntu-20.04-cuda-12.4",
      "ubuntu-20.04-cuda-12.4-docker",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.0",
      "ubuntu-22.04-cuda-12.0-docker",
      "ubuntu-22.04-cuda-12.1",
      "ubuntu-22.04-cuda-12.1-docker",
      "ubuntu-22.04-cuda-12.3",
      "ubuntu-22.04-cuda-12.3-docker",
      "ubuntu-22.04-cuda-12.4",
      "ubuntu-22.04-cuda-12.4-cluster",
      "ubuntu-22.04-cuda-12.4-docker",
      "ubuntu-22.04-cuda-12.8-cluster",
      "ubuntu-22.04-cuda-12.8-open",
      "ubuntu-22.04-cuda-12.8-open-docker",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.6",
      "ubuntu-24.04-cuda-12.6-docker",
      "ubuntu-24.04-cuda-12.8-open",
      "ubuntu-24.04-cuda-12.8-open-docker"
    ]
  },
  {
    "best_for": [
      "Giant ML models",
      "Multi-GPU training",
      "FP64 calculations",
      "NVLINK"
    ],
    "cpu": {
      "description": "22 CPU",
      "number_of_cores": 22
    },
    "deploy_warning": null,
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "1x A100 SXM4 40GB",
      "number_of_gpus": 1
    },
    "gpu_memory": {
      "description": "40GB GPU RAM",
      "size_in_gigabytes": 40
    },
    "id": "a1000000-a5d2-4972-ae4e-d429115d055b",
    "instance_type": "1A100.40S.22V",
    "memory": {
      "description": "120GB RAM",
      "size_in_gigabytes": 120
    },
    "model": "A100 40GB",
    "name": "A100 SXM4 40GB",
    "p2p": null,
    "price_per_hour": "0.7211",
    "spot_price": "0.2524",
    "dynamic_price": "0.7211",
    "max_dynamic_price": "1.082",
    "serverless_price": "0.7932",
    "serverless_spot_price": "0.2776",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "NVIDIA",
    "display_name": null,
    "supported_os": [
      "24.04.kubernetes1.31.1.cuda12.9.qcow2",
      "jupyter",
      "jupyter.cuda.12.8",
      "ubuntu-20.04",
      "ubuntu-20.04-cuda-11.0",
      "ubuntu-20.04-cuda-11.1",
      "ubuntu-20.04-cuda-11.1-docker",
      "ubuntu-20.04-cuda-11.2",
      "ubuntu-20.04-cuda-11.4",
      "ubuntu-20.04-cuda-11.4-docker",
      "ubuntu-20.04-cuda-11.7",
      "ubuntu-20.04-cuda-11.7-docker",
      "ubuntu-20.04-cuda-12.4",
      "ubuntu-20.04-cuda-12.4-docker",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.0",
      "ubuntu-22.04-cuda-12.0-docker",
      "ubuntu-22.04-cuda-12.1",
      "ubuntu-22.04-cuda-12.1-docker",
      "ubuntu-22.04-cuda-12.3",
      "ubuntu-22.04-cuda-12.3-docker",
      "ubuntu-22.04-cuda-12.4",
      "ubuntu-22.04-cuda-12.4-cluster",
      "ubuntu-22.04-cuda-12.4-docker",
      "ubuntu-22.04-cuda-12.8-cluster",
      "ubuntu-22.04-cuda-12.8-open",
      "ubuntu-22.04-cuda-12.8-open-docker",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.6",
      "ubuntu-24.04-cuda-12.6-docker",
      "ubuntu-24.04-cuda-12.8-open",
      "ubuntu-24.04-cuda-12.8-open-docker"
    ]
  },
  {
    "best_for": [
      "Giant ML models",
      "Multi-GPU training",
      "FP64 calculations",
      "NVLINK"
    ],
    "cpu": {
      "description": "176 CPU",
      "number_of_cores": 176
    },
    "deploy_warning": null,
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "8x A100 SXM4 40GB",
      "number_of_gpus": 8
    },
    "gpu_memory": {
      "description": "320GB GPU RAM",
      "size_in_gigabytes": 320
    },
    "id": "a1000003-a5d2-4972-ae4e-d429115d055b",
    "instance_type": "8A100.40S.176V",
    "memory": {
      "description": "960GB RAM",
      "size_in_gigabytes": 960
    },
    "model": "A100 40GB",
    "name": "A100 SXM4 40GB",
    "p2p": "600 GB/s",
    "price_per_hour": "5.769",
    "spot_price": "2.019",
    "dynamic_price": "5.769",
    "max_dynamic_price": "8.654",
    "serverless_price": "6.346",
    "serverless_spot_price": "2.221",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "NVIDIA",
    "display_name": null,
    "supported_os": [
      "24.04.kubernetes1.31.1.cuda12.9.qcow2",
      "jupyter",
      "jupyter.cuda.12.8",
      "ubuntu-20.04",
      "ubuntu-20.04-cuda-11.0",
      "ubuntu-20.04-cuda-11.1",
      "ubuntu-20.04-cuda-11.1-docker",
      "ubuntu-20.04-cuda-11.2",
      "ubuntu-20.04-cuda-11.4",
      "ubuntu-20.04-cuda-11.4-docker",
      "ubuntu-20.04-cuda-11.7",
      "ubuntu-20.04-cuda-11.7-docker",
      "ubuntu-20.04-cuda-12.4",
      "ubuntu-20.04-cuda-12.4-docker",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.0",
      "ubuntu-22.04-cuda-12.0-docker",
      "ubuntu-22.04-cuda-12.1",
      "ubuntu-22.04-cuda-12.1-docker",
      "ubuntu-22.04-cuda-12.3",
      "ubuntu-22.04-cuda-12.3-docker",
      "ubuntu-22.04-cuda-12.4",
      "ubuntu-22.04-cuda-12.4-cluster",
      "ubuntu-22.04-cuda-12.4-docker",
      "ubuntu-22.04-cuda-12.8-cluster",
      "ubuntu-22.04-cuda-12.8-open",
      "ubuntu-22.04-cuda-12.8-open-docker",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.6",
      "ubuntu-24.04-cuda-12.6-docker",
      "ubuntu-24.04-cuda-12.8-open",
      "ubuntu-24.04-cuda-12.8-open-docker"
    ]
  },
  {
    "best_for": [
      "Small ML models",
      "Multi-GPU training",
      "FP64 calculations",
      "NVLINK"
    ],
    "cpu": {
      "description": "6 CPU",
      "number_of_cores": 6
    },
    "deploy_warning": null,
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "1x Tesla V100 16GB",
      "number_of_gpus": 1
    },
    "gpu_memory": {
      "description": "16GB GPU RAM",
      "size_in_gigabytes": 16
    },
    "id": "04cf5dc1-a5d2-4972-ae4e-d429115d055b",
    "instance_type": "1V100.6V",
    "memory": {
      "description": "23GB RAM",
      "size_in_gigabytes": 23
    },
    "model": "Tesla V100",
    "name": "Tesla V100 16GB",
    "p2p": null,
    "price_per_hour": "0.1381",
    "spot_price": "0.04830",
    "dynamic_price": "0.1381",
    "max_dynamic_price": "0.2072",
    "serverless_price": "0.1519",
    "serverless_spot_price": "0.05320",
    "storage": {
      "description": "225GB NVME",
      "size_in_gigabytes": 225
    },
    "currency": "usd",
    "manufacturer": "NVIDIA",
    "display_name": null,
    "supported_os": [
      "jupyter",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.0",
      "ubuntu-22.04-cuda-12.0-docker",
      "ubuntu-22.04-cuda-12.1",
      "ubuntu-22.04-cuda-12.1-docker",
      "ubuntu-22.04-cuda-12.3",
      "ubuntu-22.04-cuda-12.3-docker",
      "ubuntu-22.04-cuda-12.4",
      "ubuntu-22.04-cuda-12.4-cluster",
      "ubuntu-22.04-cuda-12.4-docker",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.6",
      "ubuntu-24.04-cuda-12.6-docker"
    ]
  },
  {
    "best_for": [
      "Small ML models",
      "Multi-GPU training",
      "FP64 calculations",
      "NVLINK"
    ],
    "cpu": {
      "description": "10 CPU",
      "number_of_cores": 10
    },
    "deploy_warning": null,
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "2x Tesla V100 16GB",
      "number_of_gpus": 2
    },
    "gpu_memory": {
      "description": "32GB GPU RAM",
      "size_in_gigabytes": 32
    },
    "id": "03cf5dc1-a5d2-4972-ae4e-d429115d055b",
    "instance_type": "2V100.10V",
    "memory": {
      "description": "45GB RAM",
      "size_in_gigabytes": 45
    },
    "model": "Tesla V100",
    "name": "Tesla V100 16GB",
    "p2p": "NVLink up to 50GB/s",
    "price_per_hour": "0.2762",
    "spot_price": "0.09660",
    "dynamic_price": "0.2762",
    "max_dynamic_price": "0.4144",
    "serverless_price": "0.3038",
    "serverless_spot_price": "0.1064",
    "storage": {
      "description": "450GB NVME",
      "size_in_gigabytes": 450
    },
    "currency": "usd",
    "manufacturer": "NVIDIA",
    "display_name": null,
    "supported_os": [
      "jupyter",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.0",
      "ubuntu-22.04-cuda-12.0-docker",
      "ubuntu-22.04-cuda-12.1",
      "ubuntu-22.04-cuda-12.1-docker",
      "ubuntu-22.04-cuda-12.3",
      "ubuntu-22.04-cuda-12.3-docker",
      "ubuntu-22.04-cuda-12.4",
      "ubuntu-22.04-cuda-12.4-cluster",
      "ubuntu-22.04-cuda-12.4-docker",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.6",
      "ubuntu-24.04-cuda-12.6-docker"
    ]
  },
  {
    "best_for": [
      "Small ML models",
      "Multi-GPU training",
      "FP64 calculations",
      "NVLINK"
    ],
    "cpu": {
      "description": "20 CPU",
      "number_of_cores": 20
    },
    "deploy_warning": null,
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "4x Tesla V100 16GB",
      "number_of_gpus": 4
    },
    "gpu_memory": {
      "description": "64GB GPU RAM",
      "size_in_gigabytes": 64
    },
    "id": "02cf5dc1-a5d2-4972-ae4e-d429115d055b",
    "instance_type": "4V100.20V",
    "memory": {
      "description": "90GB RAM",
      "size_in_gigabytes": 90
    },
    "model": "Tesla V100",
    "name": "Tesla V100 16GB",
    "p2p": "NVLink up to 50GB/s",
    "price_per_hour": "0.5524",
    "spot_price": "0.1932",
    "dynamic_price": "0.5524",
    "max_dynamic_price": "0.8288",
    "serverless_price": "0.6076",
    "serverless_spot_price": "0.2128",
    "storage": {
      "description": "890GB NVME",
      "size_in_gigabytes": 890
    },
    "currency": "usd",
    "manufacturer": "NVIDIA",
    "display_name": null,
    "supported_os": [
      "jupyter",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.0",
      "ubuntu-22.04-cuda-12.0-docker",
      "ubuntu-22.04-cuda-12.1",
      "ubuntu-22.04-cuda-12.1-docker",
      "ubuntu-22.04-cuda-12.3",
      "ubuntu-22.04-cuda-12.3-docker",
      "ubuntu-22.04-cuda-12.4",
      "ubuntu-22.04-cuda-12.4-cluster",
      "ubuntu-22.04-cuda-12.4-docker",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.6",
      "ubuntu-24.04-cuda-12.6-docker"
    ]
  },
  {
    "best_for": [
      "Small ML models",
      "Multi-GPU training",
      "FP64 calculations",
      "NVLINK"
    ],
    "cpu": {
      "description": "48 CPU",
      "number_of_cores": 48
    },
    "deploy_warning": null,
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "8x Tesla V100 16GB",
      "number_of_gpus": 8
    },
    "gpu_memory": {
      "description": "128GB GPU RAM",
      "size_in_gigabytes": 128
    },
    "id": "05cf5dc1-a5d2-4972-ae4e-d429115d055b",
    "instance_type": "8V100.48V",
    "memory": {
      "description": "180GB RAM",
      "size_in_gigabytes": 180
    },
    "model": "Tesla V100",
    "name": "Tesla V100 16GB",
    "p2p": "NVLink up to 50GB/s",
    "price_per_hour": "1.105",
    "spot_price": "0.3864",
    "dynamic_price": "1.105",
    "max_dynamic_price": "1.658",
    "serverless_price": "1.215",
    "serverless_spot_price": "0.4256",
    "storage": {
      "description": "1800GB NVME",
      "size_in_gigabytes": 1800
    },
    "currency": "usd",
    "manufacturer": "NVIDIA",
    "display_name": null,
    "supported_os": [
      "jupyter",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.0",
      "ubuntu-22.04-cuda-12.0-docker",
      "ubuntu-22.04-cuda-12.1",
      "ubuntu-22.04-cuda-12.1-docker",
      "ubuntu-22.04-cuda-12.3",
      "ubuntu-22.04-cuda-12.3-docker",
      "ubuntu-22.04-cuda-12.4",
      "ubuntu-22.04-cuda-12.4-cluster",
      "ubuntu-22.04-cuda-12.4-docker",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.6",
      "ubuntu-24.04-cuda-12.6-docker"
    ]
  },
  {
    "best_for": [],
    "cpu": {
      "description": "30 CPU",
      "number_of_cores": 30
    },
    "deploy_warning": null,
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "1x RTX PRO 6000 96GB",
      "number_of_gpus": 1
    },
    "gpu_memory": {
      "description": "96GB GPU RAM",
      "size_in_gigabytes": 96
    },
    "id": "60006000-6000-47af-8ff6-600060006001",
    "instance_type": "1RTXPRO6000.30V",
    "memory": {
      "description": "90GB RAM",
      "size_in_gigabytes": 90
    },
    "model": "RTX PRO 6000",
    "name": "RTX PRO 6000 96GB",
    "p2p": null,
    "price_per_hour": "1.390",
    "spot_price": "0.4865",
    "dynamic_price": "1.390",
    "max_dynamic_price": "2.085",
    "serverless_price": "1.529",
    "serverless_spot_price": "0.5352",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "NVIDIA",
    "display_name": null,
    "supported_os": [
      "24.04.kubernetes1.31.1.cuda12.9.qcow2",
      "jupyter.cuda.12.8",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.8-open",
      "ubuntu-22.04-cuda-12.8-open-docker",
      "ubuntu-22.04-cuda-13.0-open",
      "ubuntu-22.04-cuda-13.0-open-docker",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.8-open",
      "ubuntu-24.04-cuda-12.8-open-docker",
      "ubuntu-24.04-cuda-13.0-open",
      "ubuntu-24.04-cuda-13.0-open-docker"
    ]
  },
  {
    "best_for": [],
    "cpu": {
      "description": "60 CPU",
      "number_of_cores": 60
    },
    "deploy_warning": null,
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "2x RTX PRO 6000 96GB",
      "number_of_gpus": 2
    },
    "gpu_memory": {
      "description": "192GB GPU RAM",
      "size_in_gigabytes": 192
    },
    "id": "60006000-6000-47af-8ff6-600060006002",
    "instance_type": "2RTXPRO6000.60V",
    "memory": {
      "description": "180GB RAM",
      "size_in_gigabytes": 180
    },
    "model": "RTX PRO 6000",
    "name": "RTX PRO 6000 96GB",
    "p2p": "1597 GB/s",
    "price_per_hour": "2.780",
    "spot_price": "0.9730",
    "dynamic_price": "2.780",
    "max_dynamic_price": "4.170",
    "serverless_price": "3.058",
    "serverless_spot_price": "1.070",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "NVIDIA",
    "display_name": null,
    "supported_os": [
      "24.04.kubernetes1.31.1.cuda12.9.qcow2",
      "jupyter.cuda.12.8",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.8-open",
      "ubuntu-22.04-cuda-12.8-open-docker",
      "ubuntu-22.04-cuda-13.0-open",
      "ubuntu-22.04-cuda-13.0-open-docker",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.8-open",
      "ubuntu-24.04-cuda-12.8-open-docker",
      "ubuntu-24.04-cuda-13.0-open",
      "ubuntu-24.04-cuda-13.0-open-docker"
    ]
  },
  {
    "best_for": [],
    "cpu": {
      "description": "120 CPU",
      "number_of_cores": 120
    },
    "deploy_warning": null,
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "4x RTX PRO 6000 96GB",
      "number_of_gpus": 4
    },
    "gpu_memory": {
      "description": "384GB GPU RAM",
      "size_in_gigabytes": 384
    },
    "id": "60006000-6000-47af-8ff6-600060006003",
    "instance_type": "4RTXPRO6000.120V",
    "memory": {
      "description": "360GB RAM",
      "size_in_gigabytes": 360
    },
    "model": "RTX PRO 6000",
    "name": "RTX PRO 6000 96GB",
    "p2p": "1597 GB/s",
    "price_per_hour": "5.560",
    "spot_price": "1.946",
    "dynamic_price": "5.560",
    "max_dynamic_price": "8.340",
    "serverless_price": "6.116",
    "serverless_spot_price": "2.141",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "NVIDIA",
    "display_name": null,
    "supported_os": [
      "24.04.kubernetes1.31.1.cuda12.9.qcow2",
      "jupyter.cuda.12.8",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.8-open",
      "ubuntu-22.04-cuda-12.8-open-docker",
      "ubuntu-22.04-cuda-13.0-open",
      "ubuntu-22.04-cuda-13.0-open-docker",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.8-open",
      "ubuntu-24.04-cuda-12.8-open-docker",
      "ubuntu-24.04-cuda-13.0-open",
      "ubuntu-24.04-cuda-13.0-open-docker"
    ]
  },
  {
    "best_for": [],
    "cpu": {
      "description": "240 CPU",
      "number_of_cores": 240
    },
    "deploy_warning": null,
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "8x RTX PRO 6000 96GB",
      "number_of_gpus": 8
    },
    "gpu_memory": {
      "description": "768GB GPU RAM",
      "size_in_gigabytes": 768
    },
    "id": "60006000-6000-47af-8ff6-600060006004",
    "instance_type": "8RTXPRO6000.240V",
    "memory": {
      "description": "720GB RAM",
      "size_in_gigabytes": 720
    },
    "model": "RTX PRO 6000",
    "name": "RTX PRO 6000 96GB",
    "p2p": "1597 GB/s",
    "price_per_hour": "11.12",
    "spot_price": "3.892",
    "dynamic_price": "11.12",
    "max_dynamic_price": "16.68",
    "serverless_price": "12.23",
    "serverless_spot_price": "4.282",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "NVIDIA",
    "display_name": null,
    "supported_os": [
      "24.04.kubernetes1.31.1.cuda12.9.qcow2",
      "jupyter.cuda.12.8",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.8-open",
      "ubuntu-22.04-cuda-12.8-open-docker",
      "ubuntu-22.04-cuda-13.0-open",
      "ubuntu-22.04-cuda-13.0-open-docker",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.8-open",
      "ubuntu-24.04-cuda-12.8-open-docker",
      "ubuntu-24.04-cuda-13.0-open",
      "ubuntu-24.04-cuda-13.0-open-docker"
    ]
  },
  {
    "best_for": [
      "Large ML models",
      "32-16-8 bit operations",
      "Single-GPU training"
    ],
    "cpu": {
      "description": "20 CPU",
      "number_of_cores": 20
    },
    "deploy_warning": null,
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "1x L40S 48GB",
      "number_of_gpus": 1
    },
    "gpu_memory": {
      "description": "48GB GPU RAM",
      "size_in_gigabytes": 48
    },
    "id": "40000000-a5d2-4972-ae4e-d429115d055b",
    "instance_type": "1L40S.20V",
    "memory": {
      "description": "60GB RAM",
      "size_in_gigabytes": 60
    },
    "model": "L40S",
    "name": "L40S 48GB",
    "p2p": null,
    "price_per_hour": "0.9143",
    "spot_price": "0.3200",
    "dynamic_price": "0.9143",
    "max_dynamic_price": "1.372",
    "serverless_price": "1.006",
    "serverless_spot_price": "0.3520",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "NVIDIA",
    "display_name": null,
    "supported_os": [
      "jupyter",
      "jupyter.cuda.12.8",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.0",
      "ubuntu-22.04-cuda-12.0-docker",
      "ubuntu-22.04-cuda-12.1",
      "ubuntu-22.04-cuda-12.1-docker",
      "ubuntu-22.04-cuda-12.3",
      "ubuntu-22.04-cuda-12.3-docker",
      "ubuntu-22.04-cuda-12.4",
      "ubuntu-22.04-cuda-12.4-cluster",
      "ubuntu-22.04-cuda-12.4-docker",
      "ubuntu-22.04-cuda-12.8-cluster",
      "ubuntu-22.04-cuda-12.8-open",
      "ubuntu-22.04-cuda-12.8-open-docker",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.6",
      "ubuntu-24.04-cuda-12.6-docker",
      "ubuntu-24.04-cuda-12.8-open",
      "ubuntu-24.04-cuda-12.8-open-docker"
    ]
  },
  {
    "best_for": [
      "Large ML models",
      "32-16-8 bit operations",
      "Single-GPU training"
    ],
    "cpu": {
      "description": "40 CPU",
      "number_of_cores": 40
    },
    "deploy_warning": null,
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "2x L40S 48GB",
      "number_of_gpus": 2
    },
    "gpu_memory": {
      "description": "96GB GPU RAM",
      "size_in_gigabytes": 96
    },
    "id": "40000001-a5d2-4972-ae4e-d429115d055b",
    "instance_type": "2L40S.40V",
    "memory": {
      "description": "120GB RAM",
      "size_in_gigabytes": 120
    },
    "model": "L40S",
    "name": "L40S 48GB",
    "p2p": "50GB/s",
    "price_per_hour": "1.829",
    "spot_price": "0.6400",
    "dynamic_price": "1.829",
    "max_dynamic_price": "2.743",
    "serverless_price": "2.011",
    "serverless_spot_price": "0.7040",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "NVIDIA",
    "display_name": null,
    "supported_os": [
      "jupyter",
      "jupyter.cuda.12.8",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.0",
      "ubuntu-22.04-cuda-12.0-docker",
      "ubuntu-22.04-cuda-12.1",
      "ubuntu-22.04-cuda-12.1-docker",
      "ubuntu-22.04-cuda-12.3",
      "ubuntu-22.04-cuda-12.3-docker",
      "ubuntu-22.04-cuda-12.4",
      "ubuntu-22.04-cuda-12.4-cluster",
      "ubuntu-22.04-cuda-12.4-docker",
      "ubuntu-22.04-cuda-12.8-cluster",
      "ubuntu-22.04-cuda-12.8-open",
      "ubuntu-22.04-cuda-12.8-open-docker",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.6",
      "ubuntu-24.04-cuda-12.6-docker",
      "ubuntu-24.04-cuda-12.8-open",
      "ubuntu-24.04-cuda-12.8-open-docker"
    ]
  },
  {
    "best_for": [
      "Large ML models",
      "32-16-8 bit operations",
      "Single-GPU training"
    ],
    "cpu": {
      "description": "80 CPU",
      "number_of_cores": 80
    },
    "deploy_warning": null,
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "4x L40S 48GB",
      "number_of_gpus": 4
    },
    "gpu_memory": {
      "description": "192GB GPU RAM",
      "size_in_gigabytes": 192
    },
    "id": "40000002-a5d2-4972-ae4e-d429115d055b",
    "instance_type": "4L40S.80V",
    "memory": {
      "description": "240GB RAM",
      "size_in_gigabytes": 240
    },
    "model": "L40S",
    "name": "L40S 48GB",
    "p2p": "50GB/s",
    "price_per_hour": "3.657",
    "spot_price": "1.280",
    "dynamic_price": "3.657",
    "max_dynamic_price": "5.486",
    "serverless_price": "4.023",
    "serverless_spot_price": "1.408",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "NVIDIA",
    "display_name": null,
    "supported_os": [
      "jupyter",
      "jupyter.cuda.12.8",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.0",
      "ubuntu-22.04-cuda-12.0-docker",
      "ubuntu-22.04-cuda-12.1",
      "ubuntu-22.04-cuda-12.1-docker",
      "ubuntu-22.04-cuda-12.3",
      "ubuntu-22.04-cuda-12.3-docker",
      "ubuntu-22.04-cuda-12.4",
      "ubuntu-22.04-cuda-12.4-cluster",
      "ubuntu-22.04-cuda-12.4-docker",
      "ubuntu-22.04-cuda-12.8-cluster",
      "ubuntu-22.04-cuda-12.8-open",
      "ubuntu-22.04-cuda-12.8-open-docker",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.6",
      "ubuntu-24.04-cuda-12.6-docker",
      "ubuntu-24.04-cuda-12.8-open",
      "ubuntu-24.04-cuda-12.8-open-docker"
    ]
  },
  {
    "best_for": [
      "Large ML models",
      "32-16-8 bit operations",
      "Single-GPU training"
    ],
    "cpu": {
      "description": "160 CPU",
      "number_of_cores": 160
    },
    "deploy_warning": null,
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "8x L40S 48GB",
      "number_of_gpus": 8
    },
    "gpu_memory": {
      "description": "384GB GPU RAM",
      "size_in_gigabytes": 384
    },
    "id": "40000003-a5d2-4972-ae4e-d429115d055b",
    "instance_type": "8L40S.160V",
    "memory": {
      "description": "480GB RAM",
      "size_in_gigabytes": 480
    },
    "model": "L40S",
    "name": "L40S 48GB",
    "p2p": "50GB/s",
    "price_per_hour": "7.314",
    "spot_price": "2.560",
    "dynamic_price": "7.314",
    "max_dynamic_price": "10.97",
    "serverless_price": "8.046",
    "serverless_spot_price": "2.816",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "NVIDIA",
    "display_name": null,
    "supported_os": [
      "jupyter",
      "jupyter.cuda.12.8",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.0",
      "ubuntu-22.04-cuda-12.0-docker",
      "ubuntu-22.04-cuda-12.1",
      "ubuntu-22.04-cuda-12.1-docker",
      "ubuntu-22.04-cuda-12.3",
      "ubuntu-22.04-cuda-12.3-docker",
      "ubuntu-22.04-cuda-12.4",
      "ubuntu-22.04-cuda-12.4-cluster",
      "ubuntu-22.04-cuda-12.4-docker",
      "ubuntu-22.04-cuda-12.8-cluster",
      "ubuntu-22.04-cuda-12.8-open",
      "ubuntu-22.04-cuda-12.8-open-docker",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.6",
      "ubuntu-24.04-cuda-12.6-docker",
      "ubuntu-24.04-cuda-12.8-open",
      "ubuntu-24.04-cuda-12.8-open-docker"
    ]
  },
  {
    "best_for": [
      "Large ML models",
      "32-16-8 bit operations",
      "Single-GPU training"
    ],
    "cpu": {
      "description": "10 CPU",
      "number_of_cores": 10
    },
    "deploy_warning": "RTX6000 Ada: Make sure your VM is running Nvidia driver 525.105.17 or newer",
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "1x RTX 6000 Ada 48GB",
      "number_of_gpus": 1
    },
    "gpu_memory": {
      "description": "48GB GPU RAM",
      "size_in_gigabytes": 48
    },
    "id": "a6000ada-0000-4972-ae4e-d429115d055b",
    "instance_type": "1RTX6000ADA.10V",
    "memory": {
      "description": "60GB RAM",
      "size_in_gigabytes": 60
    },
    "model": "RTX 6000 Ada",
    "name": "RTX 6000 Ada 48GB",
    "p2p": null,
    "price_per_hour": "0.8262",
    "spot_price": "0.2892",
    "dynamic_price": "0.8262",
    "max_dynamic_price": "1.239",
    "serverless_price": "0.9088",
    "serverless_spot_price": "0.3181",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "NVIDIA",
    "display_name": null,
    "supported_os": [
      "jupyter",
      "jupyter.cuda.12.8",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.0",
      "ubuntu-22.04-cuda-12.0-docker",
      "ubuntu-22.04-cuda-12.1",
      "ubuntu-22.04-cuda-12.1-docker",
      "ubuntu-22.04-cuda-12.3",
      "ubuntu-22.04-cuda-12.3-docker",
      "ubuntu-22.04-cuda-12.4",
      "ubuntu-22.04-cuda-12.4-cluster",
      "ubuntu-22.04-cuda-12.4-docker",
      "ubuntu-22.04-cuda-12.8-cluster",
      "ubuntu-22.04-cuda-12.8-open",
      "ubuntu-22.04-cuda-12.8-open-docker",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.6",
      "ubuntu-24.04-cuda-12.6-docker",
      "ubuntu-24.04-cuda-12.8-open",
      "ubuntu-24.04-cuda-12.8-open-docker"
    ]
  },
  {
    "best_for": [
      "Large ML models",
      "32-16-8 bit operations",
      "Single-GPU training"
    ],
    "cpu": {
      "description": "20 CPU",
      "number_of_cores": 20
    },
    "deploy_warning": "RTX6000 Ada: Make sure your VM is running Nvidia driver 525.105.17 or newer",
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "2x RTX 6000 Ada 48GB",
      "number_of_gpus": 2
    },
    "gpu_memory": {
      "description": "96GB GPU RAM",
      "size_in_gigabytes": 96
    },
    "id": "a6000ada-0001-4972-ae4e-d429115d055b",
    "instance_type": "2RTX6000ADA.20V",
    "memory": {
      "description": "120GB RAM",
      "size_in_gigabytes": 120
    },
    "model": "RTX 6000 Ada",
    "name": "RTX 6000 Ada 48GB",
    "p2p": "50GB/s",
    "price_per_hour": "1.652",
    "spot_price": "0.5784",
    "dynamic_price": "1.652",
    "max_dynamic_price": "2.479",
    "serverless_price": "1.818",
    "serverless_spot_price": "0.6362",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "NVIDIA",
    "display_name": null,
    "supported_os": [
      "jupyter",
      "jupyter.cuda.12.8",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.0",
      "ubuntu-22.04-cuda-12.0-docker",
      "ubuntu-22.04-cuda-12.1",
      "ubuntu-22.04-cuda-12.1-docker",
      "ubuntu-22.04-cuda-12.3",
      "ubuntu-22.04-cuda-12.3-docker",
      "ubuntu-22.04-cuda-12.4",
      "ubuntu-22.04-cuda-12.4-cluster",
      "ubuntu-22.04-cuda-12.4-docker",
      "ubuntu-22.04-cuda-12.8-cluster",
      "ubuntu-22.04-cuda-12.8-open",
      "ubuntu-22.04-cuda-12.8-open-docker",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.6",
      "ubuntu-24.04-cuda-12.6-docker",
      "ubuntu-24.04-cuda-12.8-open",
      "ubuntu-24.04-cuda-12.8-open-docker"
    ]
  },
  {
    "best_for": [
      "Large ML models",
      "32-16-8 bit operations",
      "Single-GPU training"
    ],
    "cpu": {
      "description": "40 CPU",
      "number_of_cores": 40
    },
    "deploy_warning": "RTX6000 Ada: Make sure your VM is running Nvidia driver 525.105.17 or newer",
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "4x RTX 6000 Ada 48GB",
      "number_of_gpus": 4
    },
    "gpu_memory": {
      "description": "192GB GPU RAM",
      "size_in_gigabytes": 192
    },
    "id": "a6000ada-0002-4972-ae4e-d429115d055b",
    "instance_type": "4RTX6000ADA.40V",
    "memory": {
      "description": "240GB RAM",
      "size_in_gigabytes": 240
    },
    "model": "RTX 6000 Ada",
    "name": "RTX 6000 Ada 48GB",
    "p2p": "50GB/s",
    "price_per_hour": "3.305",
    "spot_price": "1.157",
    "dynamic_price": "3.305",
    "max_dynamic_price": "4.957",
    "serverless_price": "3.635",
    "serverless_spot_price": "1.272",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "NVIDIA",
    "display_name": null,
    "supported_os": [
      "jupyter",
      "jupyter.cuda.12.8",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.0",
      "ubuntu-22.04-cuda-12.0-docker",
      "ubuntu-22.04-cuda-12.1",
      "ubuntu-22.04-cuda-12.1-docker",
      "ubuntu-22.04-cuda-12.3",
      "ubuntu-22.04-cuda-12.3-docker",
      "ubuntu-22.04-cuda-12.4",
      "ubuntu-22.04-cuda-12.4-cluster",
      "ubuntu-22.04-cuda-12.4-docker",
      "ubuntu-22.04-cuda-12.8-cluster",
      "ubuntu-22.04-cuda-12.8-open",
      "ubuntu-22.04-cuda-12.8-open-docker",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.6",
      "ubuntu-24.04-cuda-12.6-docker",
      "ubuntu-24.04-cuda-12.8-open",
      "ubuntu-24.04-cuda-12.8-open-docker"
    ]
  },
  {
    "best_for": [
      "Large ML models",
      "32-16-8 bit operations",
      "Single-GPU training"
    ],
    "cpu": {
      "description": "80 CPU",
      "number_of_cores": 80
    },
    "deploy_warning": "RTX6000 Ada: Make sure your VM is running Nvidia driver 525.105.17 or newer",
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "8x RTX 6000 Ada 48GB",
      "number_of_gpus": 8
    },
    "gpu_memory": {
      "description": "384GB GPU RAM",
      "size_in_gigabytes": 384
    },
    "id": "a6000ada-0003-4972-ae4e-d429115d055b",
    "instance_type": "8RTX6000ADA.80V",
    "memory": {
      "description": "480GB RAM",
      "size_in_gigabytes": 480
    },
    "model": "RTX 6000 Ada",
    "name": "RTX 6000 Ada 48GB",
    "p2p": "50GB/s",
    "price_per_hour": "6.610",
    "spot_price": "2.314",
    "dynamic_price": "6.610",
    "max_dynamic_price": "9.914",
    "serverless_price": "7.270",
    "serverless_spot_price": "2.545",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "NVIDIA",
    "display_name": null,
    "supported_os": [
      "jupyter",
      "jupyter.cuda.12.8",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.0",
      "ubuntu-22.04-cuda-12.0-docker",
      "ubuntu-22.04-cuda-12.1",
      "ubuntu-22.04-cuda-12.1-docker",
      "ubuntu-22.04-cuda-12.3",
      "ubuntu-22.04-cuda-12.3-docker",
      "ubuntu-22.04-cuda-12.4",
      "ubuntu-22.04-cuda-12.4-cluster",
      "ubuntu-22.04-cuda-12.4-docker",
      "ubuntu-22.04-cuda-12.8-cluster",
      "ubuntu-22.04-cuda-12.8-open",
      "ubuntu-22.04-cuda-12.8-open-docker",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.6",
      "ubuntu-24.04-cuda-12.6-docker",
      "ubuntu-24.04-cuda-12.8-open",
      "ubuntu-24.04-cuda-12.8-open-docker"
    ]
  },
  {
    "best_for": [
      "Large ML models",
      "FP32 calculations",
      "Single-GPU training"
    ],
    "cpu": {
      "description": "10 CPU",
      "number_of_cores": 10
    },
    "deploy_warning": null,
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "1x RTX A6000 48GB",
      "number_of_gpus": 1
    },
    "gpu_memory": {
      "description": "48GB GPU RAM",
      "size_in_gigabytes": 48
    },
    "id": "06cf5dc1-a5d2-4972-ae4e-d429115d055b",
    "instance_type": "1A6000.10V",
    "memory": {
      "description": "60GB RAM",
      "size_in_gigabytes": 60
    },
    "model": "RTX A6000",
    "name": "RTX A6000 48GB",
    "p2p": null,
    "price_per_hour": "0.4900",
    "spot_price": "0.1715",
    "dynamic_price": "0.4900",
    "max_dynamic_price": "0.7350",
    "serverless_price": "0.5390",
    "serverless_spot_price": "0.1887",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "NVIDIA",
    "display_name": null,
    "supported_os": [
      "24.04.kubernetes1.31.1.cuda12.9.qcow2",
      "jupyter",
      "jupyter.cuda.12.8",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.0",
      "ubuntu-22.04-cuda-12.0-docker",
      "ubuntu-22.04-cuda-12.1",
      "ubuntu-22.04-cuda-12.1-docker",
      "ubuntu-22.04-cuda-12.3",
      "ubuntu-22.04-cuda-12.3-docker",
      "ubuntu-22.04-cuda-12.4",
      "ubuntu-22.04-cuda-12.4-cluster",
      "ubuntu-22.04-cuda-12.4-docker",
      "ubuntu-22.04-cuda-12.8-cluster",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.6",
      "ubuntu-24.04-cuda-12.6-docker"
    ]
  },
  {
    "best_for": [
      "Large ML models",
      "FP32 calculations",
      "Single-GPU training"
    ],
    "cpu": {
      "description": "20 CPU",
      "number_of_cores": 20
    },
    "deploy_warning": null,
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "2x RTX A6000 48GB",
      "number_of_gpus": 2
    },
    "gpu_memory": {
      "description": "96GB GPU RAM",
      "size_in_gigabytes": 96
    },
    "id": "07cf5dc1-a5d2-4972-ae4e-d429115d055b",
    "instance_type": "2A6000.20V",
    "memory": {
      "description": "120GB RAM",
      "size_in_gigabytes": 120
    },
    "model": "RTX A6000",
    "name": "RTX A6000 48GB",
    "p2p": "50GB/s",
    "price_per_hour": "0.9800",
    "spot_price": "0.3430",
    "dynamic_price": "0.9800",
    "max_dynamic_price": "1.470",
    "serverless_price": "1.078",
    "serverless_spot_price": "0.3774",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "NVIDIA",
    "display_name": null,
    "supported_os": [
      "24.04.kubernetes1.31.1.cuda12.9.qcow2",
      "jupyter",
      "jupyter.cuda.12.8",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.0",
      "ubuntu-22.04-cuda-12.0-docker",
      "ubuntu-22.04-cuda-12.1",
      "ubuntu-22.04-cuda-12.1-docker",
      "ubuntu-22.04-cuda-12.3",
      "ubuntu-22.04-cuda-12.3-docker",
      "ubuntu-22.04-cuda-12.4",
      "ubuntu-22.04-cuda-12.4-cluster",
      "ubuntu-22.04-cuda-12.4-docker",
      "ubuntu-22.04-cuda-12.8-cluster",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.6",
      "ubuntu-24.04-cuda-12.6-docker"
    ]
  },
  {
    "best_for": [
      "Large ML models",
      "FP32 calculations",
      "Single-GPU training"
    ],
    "cpu": {
      "description": "40 CPU",
      "number_of_cores": 40
    },
    "deploy_warning": null,
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "4x RTX A6000 48GB",
      "number_of_gpus": 4
    },
    "gpu_memory": {
      "description": "192GB GPU RAM",
      "size_in_gigabytes": 192
    },
    "id": "08cf5dc1-a5d2-4972-ae4e-d429115d055b",
    "instance_type": "4A6000.40V",
    "memory": {
      "description": "240GB RAM",
      "size_in_gigabytes": 240
    },
    "model": "RTX A6000",
    "name": "RTX A6000 48GB",
    "p2p": "50GB/s",
    "price_per_hour": "1.960",
    "spot_price": "0.6860",
    "dynamic_price": "1.960",
    "max_dynamic_price": "2.940",
    "serverless_price": "2.156",
    "serverless_spot_price": "0.7548",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "NVIDIA",
    "display_name": null,
    "supported_os": [
      "24.04.kubernetes1.31.1.cuda12.9.qcow2",
      "jupyter",
      "jupyter.cuda.12.8",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.0",
      "ubuntu-22.04-cuda-12.0-docker",
      "ubuntu-22.04-cuda-12.1",
      "ubuntu-22.04-cuda-12.1-docker",
      "ubuntu-22.04-cuda-12.3",
      "ubuntu-22.04-cuda-12.3-docker",
      "ubuntu-22.04-cuda-12.4",
      "ubuntu-22.04-cuda-12.4-cluster",
      "ubuntu-22.04-cuda-12.4-docker",
      "ubuntu-22.04-cuda-12.8-cluster",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.6",
      "ubuntu-24.04-cuda-12.6-docker"
    ]
  },
  {
    "best_for": [
      "Large ML models",
      "FP32 calculations",
      "Single-GPU training"
    ],
    "cpu": {
      "description": "80 CPU",
      "number_of_cores": 80
    },
    "deploy_warning": null,
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "8x RTX A6000 48GB",
      "number_of_gpus": 8
    },
    "gpu_memory": {
      "description": "384GB GPU RAM",
      "size_in_gigabytes": 384
    },
    "id": "09cf5dc1-a5d2-4972-ae4e-d429115d055b",
    "instance_type": "8A6000.80V",
    "memory": {
      "description": "480GB RAM",
      "size_in_gigabytes": 480
    },
    "model": "RTX A6000",
    "name": "RTX A6000 48GB",
    "p2p": "50GB/s",
    "price_per_hour": "3.920",
    "spot_price": "1.372",
    "dynamic_price": "3.920",
    "max_dynamic_price": "5.880",
    "serverless_price": "4.312",
    "serverless_spot_price": "1.510",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "NVIDIA",
    "display_name": null,
    "supported_os": [
      "24.04.kubernetes1.31.1.cuda12.9.qcow2",
      "jupyter",
      "jupyter.cuda.12.8",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.0",
      "ubuntu-22.04-cuda-12.0-docker",
      "ubuntu-22.04-cuda-12.1",
      "ubuntu-22.04-cuda-12.1-docker",
      "ubuntu-22.04-cuda-12.3",
      "ubuntu-22.04-cuda-12.3-docker",
      "ubuntu-22.04-cuda-12.4",
      "ubuntu-22.04-cuda-12.4-cluster",
      "ubuntu-22.04-cuda-12.4-docker",
      "ubuntu-22.04-cuda-12.8-cluster",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.6",
      "ubuntu-24.04-cuda-12.6-docker"
    ]
  },
  {
    "best_for": [
      "Running services",
      "API server",
      "Data transfers"
    ],
    "cpu": {
      "description": "4 CPU",
      "number_of_cores": 4
    },
    "deploy_warning": null,
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "",
      "number_of_gpus": 0
    },
    "gpu_memory": {
      "description": "",
      "size_in_gigabytes": 0
    },
    "id": "ccc00000-a5d2-4972-ae4e-d429115d055b",
    "instance_type": "CPU.4V.16G",
    "memory": {
      "description": "16GB RAM",
      "size_in_gigabytes": 16
    },
    "model": "CPU Node",
    "name": "AMD EPYC",
    "p2p": null,
    "price_per_hour": "0.02790",
    "spot_price": "0.009800",
    "dynamic_price": "0.02790",
    "max_dynamic_price": "0.04190",
    "serverless_price": "0.03070",
    "serverless_spot_price": "0.01070",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "AMD",
    "display_name": "Rome/Milan",
    "supported_os": [
      "jupyter",
      "jupyter.cuda.12.8",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.0",
      "ubuntu-22.04-cuda-12.0-docker",
      "ubuntu-22.04-cuda-12.1",
      "ubuntu-22.04-cuda-12.1-docker",
      "ubuntu-22.04-cuda-12.3",
      "ubuntu-22.04-cuda-12.3-docker",
      "ubuntu-22.04-cuda-12.4",
      "ubuntu-22.04-cuda-12.4-cluster",
      "ubuntu-22.04-cuda-12.4-docker",
      "ubuntu-22.04-cuda-12.8-cluster",
      "ubuntu-22.04-cuda-12.8-open",
      "ubuntu-22.04-cuda-12.8-open-docker",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.6",
      "ubuntu-24.04-cuda-12.6-docker",
      "ubuntu-24.04-cuda-12.8-open",
      "ubuntu-24.04-cuda-12.8-open-docker"
    ]
  },
  {
    "best_for": [
      "Running services",
      "API server",
      "Data transfers"
    ],
    "cpu": {
      "description": "8 CPU",
      "number_of_cores": 8
    },
    "deploy_warning": null,
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "",
      "number_of_gpus": 0
    },
    "gpu_memory": {
      "description": "",
      "size_in_gigabytes": 0
    },
    "id": "ccc00001-a5d2-4972-ae4e-d429115d055b",
    "instance_type": "CPU.8V.32G",
    "memory": {
      "description": "32GB RAM",
      "size_in_gigabytes": 32
    },
    "model": "CPU Node",
    "name": "AMD EPYC",
    "p2p": null,
    "price_per_hour": "0.05580",
    "spot_price": "0.01960",
    "dynamic_price": "0.05580",
    "max_dynamic_price": "0.08380",
    "serverless_price": "0.06140",
    "serverless_spot_price": "0.02140",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "AMD",
    "display_name": "Rome/Milan",
    "supported_os": [
      "jupyter",
      "jupyter.cuda.12.8",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.0",
      "ubuntu-22.04-cuda-12.0-docker",
      "ubuntu-22.04-cuda-12.1",
      "ubuntu-22.04-cuda-12.1-docker",
      "ubuntu-22.04-cuda-12.3",
      "ubuntu-22.04-cuda-12.3-docker",
      "ubuntu-22.04-cuda-12.4",
      "ubuntu-22.04-cuda-12.4-cluster",
      "ubuntu-22.04-cuda-12.4-docker",
      "ubuntu-22.04-cuda-12.8-cluster",
      "ubuntu-22.04-cuda-12.8-open",
      "ubuntu-22.04-cuda-12.8-open-docker",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.6",
      "ubuntu-24.04-cuda-12.6-docker",
      "ubuntu-24.04-cuda-12.8-open",
      "ubuntu-24.04-cuda-12.8-open-docker"
    ]
  },
  {
    "best_for": [
      "Running services",
      "API server",
      "Data transfers"
    ],
    "cpu": {
      "description": "16 CPU",
      "number_of_cores": 16
    },
    "deploy_warning": null,
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "",
      "number_of_gpus": 0
    },
    "gpu_memory": {
      "description": "",
      "size_in_gigabytes": 0
    },
    "id": "ccc00002-a5d2-4972-ae4e-d429115d055b",
    "instance_type": "CPU.16V.64G",
    "memory": {
      "description": "64GB RAM",
      "size_in_gigabytes": 64
    },
    "model": "CPU Node",
    "name": "AMD EPYC",
    "p2p": null,
    "price_per_hour": "0.1116",
    "spot_price": "0.03920",
    "dynamic_price": "0.1116",
    "max_dynamic_price": "0.1676",
    "serverless_price": "0.1228",
    "serverless_spot_price": "0.04280",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "AMD",
    "display_name": "Rome/Milan",
    "supported_os": [
      "jupyter",
      "jupyter.cuda.12.8",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.0",
      "ubuntu-22.04-cuda-12.0-docker",
      "ubuntu-22.04-cuda-12.1",
      "ubuntu-22.04-cuda-12.1-docker",
      "ubuntu-22.04-cuda-12.3",
      "ubuntu-22.04-cuda-12.3-docker",
      "ubuntu-22.04-cuda-12.4",
      "ubuntu-22.04-cuda-12.4-cluster",
      "ubuntu-22.04-cuda-12.4-docker",
      "ubuntu-22.04-cuda-12.8-cluster",
      "ubuntu-22.04-cuda-12.8-open",
      "ubuntu-22.04-cuda-12.8-open-docker",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.6",
      "ubuntu-24.04-cuda-12.6-docker",
      "ubuntu-24.04-cuda-12.8-open",
      "ubuntu-24.04-cuda-12.8-open-docker"
    ]
  },
  {
    "best_for": [
      "Running services",
      "API server",
      "Data transfers"
    ],
    "cpu": {
      "description": "32 CPU",
      "number_of_cores": 32
    },
    "deploy_warning": null,
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "",
      "number_of_gpus": 0
    },
    "gpu_memory": {
      "description": "",
      "size_in_gigabytes": 0
    },
    "id": "ccc00003-a5d2-4972-ae4e-d429115d055b",
    "instance_type": "CPU.32V.128G",
    "memory": {
      "description": "128GB RAM",
      "size_in_gigabytes": 128
    },
    "model": "CPU Node",
    "name": "AMD EPYC",
    "p2p": null,
    "price_per_hour": "0.2232",
    "spot_price": "0.07840",
    "dynamic_price": "0.2232",
    "max_dynamic_price": "0.3352",
    "serverless_price": "0.2456",
    "serverless_spot_price": "0.08560",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "AMD",
    "display_name": "Rome/Milan",
    "supported_os": [
      "jupyter",
      "jupyter.cuda.12.8",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.0",
      "ubuntu-22.04-cuda-12.0-docker",
      "ubuntu-22.04-cuda-12.1",
      "ubuntu-22.04-cuda-12.1-docker",
      "ubuntu-22.04-cuda-12.3",
      "ubuntu-22.04-cuda-12.3-docker",
      "ubuntu-22.04-cuda-12.4",
      "ubuntu-22.04-cuda-12.4-cluster",
      "ubuntu-22.04-cuda-12.4-docker",
      "ubuntu-22.04-cuda-12.8-cluster",
      "ubuntu-22.04-cuda-12.8-open",
      "ubuntu-22.04-cuda-12.8-open-docker",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.6",
      "ubuntu-24.04-cuda-12.6-docker",
      "ubuntu-24.04-cuda-12.8-open",
      "ubuntu-24.04-cuda-12.8-open-docker"
    ]
  },
  {
    "best_for": [
      "Running services",
      "API server",
      "Data transfers"
    ],
    "cpu": {
      "description": "64 CPU",
      "number_of_cores": 64
    },
    "deploy_warning": null,
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "",
      "number_of_gpus": 0
    },
    "gpu_memory": {
      "description": "",
      "size_in_gigabytes": 0
    },
    "id": "ccc00005-a5d2-4972-ae4e-d429115d055b",
    "instance_type": "CPU.64V.256G",
    "memory": {
      "description": "256GB RAM",
      "size_in_gigabytes": 256
    },
    "model": "CPU Node",
    "name": "AMD EPYC",
    "p2p": null,
    "price_per_hour": "0.4464",
    "spot_price": "0.1568",
    "dynamic_price": "0.4464",
    "max_dynamic_price": "0.6704",
    "serverless_price": "0.4912",
    "serverless_spot_price": "0.1712",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "AMD",
    "display_name": "Rome/Milan",
    "supported_os": [
      "jupyter",
      "jupyter.cuda.12.8",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.0",
      "ubuntu-22.04-cuda-12.0-docker",
      "ubuntu-22.04-cuda-12.1",
      "ubuntu-22.04-cuda-12.1-docker",
      "ubuntu-22.04-cuda-12.3",
      "ubuntu-22.04-cuda-12.3-docker",
      "ubuntu-22.04-cuda-12.4",
      "ubuntu-22.04-cuda-12.4-cluster",
      "ubuntu-22.04-cuda-12.4-docker",
      "ubuntu-22.04-cuda-12.8-cluster",
      "ubuntu-22.04-cuda-12.8-open",
      "ubuntu-22.04-cuda-12.8-open-docker",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.6",
      "ubuntu-24.04-cuda-12.6-docker",
      "ubuntu-24.04-cuda-12.8-open",
      "ubuntu-24.04-cuda-12.8-open-docker"
    ]
  },
  {
    "best_for": [
      "Running services",
      "API server",
      "Data transfers"
    ],
    "cpu": {
      "description": "96 CPU",
      "number_of_cores": 96
    },
    "deploy_warning": null,
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "",
      "number_of_gpus": 0
    },
    "gpu_memory": {
      "description": "",
      "size_in_gigabytes": 0
    },
    "id": "ccc00006-a5d2-4972-ae4e-d429115d055b",
    "instance_type": "CPU.96V.384G",
    "memory": {
      "description": "384GB RAM",
      "size_in_gigabytes": 384
    },
    "model": "CPU Node",
    "name": "AMD EPYC",
    "p2p": null,
    "price_per_hour": "0.6696",
    "spot_price": "0.2352",
    "dynamic_price": "0.6696",
    "max_dynamic_price": "1.006",
    "serverless_price": "0.7368",
    "serverless_spot_price": "0.2568",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "AMD",
    "display_name": "Rome/Milan",
    "supported_os": [
      "jupyter",
      "jupyter.cuda.12.8",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.0",
      "ubuntu-22.04-cuda-12.0-docker",
      "ubuntu-22.04-cuda-12.1",
      "ubuntu-22.04-cuda-12.1-docker",
      "ubuntu-22.04-cuda-12.3",
      "ubuntu-22.04-cuda-12.3-docker",
      "ubuntu-22.04-cuda-12.4",
      "ubuntu-22.04-cuda-12.4-cluster",
      "ubuntu-22.04-cuda-12.4-docker",
      "ubuntu-22.04-cuda-12.8-cluster",
      "ubuntu-22.04-cuda-12.8-open",
      "ubuntu-22.04-cuda-12.8-open-docker",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.6",
      "ubuntu-24.04-cuda-12.6-docker",
      "ubuntu-24.04-cuda-12.8-open",
      "ubuntu-24.04-cuda-12.8-open-docker"
    ]
  },
  {
    "best_for": [
      "Running services",
      "API server",
      "Data transfers"
    ],
    "cpu": {
      "description": "120 CPU",
      "number_of_cores": 120
    },
    "deploy_warning": null,
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "",
      "number_of_gpus": 0
    },
    "gpu_memory": {
      "description": "",
      "size_in_gigabytes": 0
    },
    "id": "ccc00007-a5d2-4972-ae4e-d429115d055b",
    "instance_type": "CPU.120V.480G",
    "memory": {
      "description": "480GB RAM",
      "size_in_gigabytes": 480
    },
    "model": "CPU Node",
    "name": "AMD EPYC",
    "p2p": null,
    "price_per_hour": "0.8370",
    "spot_price": "0.2940",
    "dynamic_price": "0.8370",
    "max_dynamic_price": "1.257",
    "serverless_price": "0.9210",
    "serverless_spot_price": "0.3210",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "AMD",
    "display_name": "Rome/Milan",
    "supported_os": [
      "jupyter",
      "jupyter.cuda.12.8",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.0",
      "ubuntu-22.04-cuda-12.0-docker",
      "ubuntu-22.04-cuda-12.1",
      "ubuntu-22.04-cuda-12.1-docker",
      "ubuntu-22.04-cuda-12.3",
      "ubuntu-22.04-cuda-12.3-docker",
      "ubuntu-22.04-cuda-12.4",
      "ubuntu-22.04-cuda-12.4-cluster",
      "ubuntu-22.04-cuda-12.4-docker",
      "ubuntu-22.04-cuda-12.8-cluster",
      "ubuntu-22.04-cuda-12.8-open",
      "ubuntu-22.04-cuda-12.8-open-docker",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.6",
      "ubuntu-24.04-cuda-12.6-docker",
      "ubuntu-24.04-cuda-12.8-open",
      "ubuntu-24.04-cuda-12.8-open-docker"
    ]
  },
  {
    "best_for": [
      "Running services",
      "API server",
      "Data transfers"
    ],
    "cpu": {
      "description": "180 CPU",
      "number_of_cores": 180
    },
    "deploy_warning": "",
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "",
      "number_of_gpus": 0
    },
    "gpu_memory": {
      "description": "",
      "size_in_gigabytes": 0
    },
    "id": "ccc10002-a5d2-4972-ae4e-d429115d055b",
    "instance_type": "CPU.180V.720G",
    "memory": {
      "description": "720GB RAM",
      "size_in_gigabytes": 720
    },
    "model": "CPU Node",
    "name": "AMD EPYC",
    "p2p": "",
    "price_per_hour": "1.256",
    "spot_price": "0.4410",
    "dynamic_price": "1.256",
    "max_dynamic_price": "1.886",
    "serverless_price": "1.382",
    "serverless_spot_price": "0.4815",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "AMD",
    "display_name": "Genoa",
    "supported_os": [
      "jupyter",
      "jupyter.cuda.12.8",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.0",
      "ubuntu-22.04-cuda-12.0-docker",
      "ubuntu-22.04-cuda-12.1",
      "ubuntu-22.04-cuda-12.1-docker",
      "ubuntu-22.04-cuda-12.3",
      "ubuntu-22.04-cuda-12.3-docker",
      "ubuntu-22.04-cuda-12.4",
      "ubuntu-22.04-cuda-12.4-cluster",
      "ubuntu-22.04-cuda-12.4-docker",
      "ubuntu-22.04-cuda-12.8-cluster",
      "ubuntu-22.04-cuda-12.8-open",
      "ubuntu-22.04-cuda-12.8-open-docker",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.6",
      "ubuntu-24.04-cuda-12.6-docker",
      "ubuntu-24.04-cuda-12.8-open",
      "ubuntu-24.04-cuda-12.8-open-docker"
    ]
  },
  {
    "best_for": [
      "Running services",
      "API server",
      "Data transfers"
    ],
    "cpu": {
      "description": "360 CPU",
      "number_of_cores": 360
    },
    "deploy_warning": "",
    "description": "Dedicated Hardware Instance",
    "gpu": {
      "description": "",
      "number_of_gpus": 0
    },
    "gpu_memory": {
      "description": "",
      "size_in_gigabytes": 0
    },
    "id": "ccc10001-a5d2-4972-ae4e-d429115d055b",
    "instance_type": "CPU.360V.1440G",
    "memory": {
      "description": "1440GB RAM",
      "size_in_gigabytes": 1440
    },
    "model": "CPU Node",
    "name": "AMD EPYC",
    "p2p": "",
    "price_per_hour": "2.511",
    "spot_price": "0.8820",
    "dynamic_price": "2.511",
    "max_dynamic_price": "3.771",
    "serverless_price": "2.763",
    "serverless_spot_price": "0.9630",
    "storage": {
      "description": "dynamic"
    },
    "currency": "usd",
    "manufacturer": "AMD",
    "display_name": "Genoa",
    "supported_os": [
      "jupyter",
      "jupyter.cuda.12.8",
      "ubuntu-22.04",
      "ubuntu-22.04-cuda-12.0",
      "ubuntu-22.04-cuda-12.0-docker",
      "ubuntu-22.04-cuda-12.1",
      "ubuntu-22.04-cuda-12.1-docker",
      "ubuntu-22.04-cuda-12.3",
      "ubuntu-22.04-cuda-12.3-docker",
      "ubuntu-22.04-cuda-12.4",
      "ubuntu-22.04-cuda-12.4-cluster",
      "ubuntu-22.04-cuda-12.4-docker",
      "ubuntu-22.04-cuda-12.8-cluster",
      "ubuntu-22.04-cuda-12.8-open",
      "ubuntu-22.04-cuda-12.8-open-docker",
      "ubuntu-24.04",
      "ubuntu-24.04-cuda-12.6",
      "ubuntu-24.04-cuda-12.6-docker",
      "ubuntu-24.04-cuda-12.8-open",
      "ubuntu-24.04-cuda-12.8-open-docker"
    ]
  }
]
